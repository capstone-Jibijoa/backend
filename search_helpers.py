import os
import re
import logging
import threading
from typing import List, Set, Optional, Dict, Tuple
from datetime import datetime
from collections import defaultdict
from functools import lru_cache
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchAny, SearchParams
from langchain_huggingface import HuggingFaceEmbeddings

from db import get_db_connection_context
from mapping_rules import CATEGORY_MAPPING, get_field_mapping

# LLM í•„ë“œëª… -> ì‹¤ì œ DB í•„ë“œëª… ë§¤í•‘
FIELD_ALIAS_MAP = {
    "household_size": "family_size",  
    "age": "birth_year",              
    "job": "job_title_raw",           
    "region": "region_major"          
}

# [í•µì‹¬ ìˆ˜ì •] ì‹¤ì œ ë°ì´í„°(ëª…ì‚¬í˜•)ë¥¼ í¬í•¨í•˜ë„ë¡ ë§¤í•‘ í‚¤ì›Œë“œ í™•ìž¥
VALUE_TRANSLATION_MAP = {
    'gender': {
        'ë‚¨ì„±': 'M', 'ì—¬ì„±': 'F', 'ë‚¨ìž': 'M', 'ì—¬ìž': 'F',
    },
    'marital_status': {
        'ë¯¸í˜¼': 'ë¯¸í˜¼', 'ì‹±ê¸€': 'ë¯¸í˜¼', 'ê¸°í˜¼': 'ê¸°í˜¼', 'ê²°í˜¼': 'ê¸°í˜¼', 'ì´í˜¼': 'ì´í˜¼', 'ëŒì‹±': 'ì´í˜¼'
    },
    'car_ownership': {
        'ìžˆìŒ': 'ìžˆë‹¤', 
        'ë³´ìœ ': 'ìžˆë‹¤', 
        'ìžì°¨': 'ìžˆë‹¤', 
        'ì˜¤ë„ˆ': 'ìžˆë‹¤',
        'ì—†ìŒ': 'ì—†ë‹¤', 
        'ë¯¸ë³´ìœ ': 'ì—†ë‹¤', 
        'ëšœë²…ì´': 'ì—†ë‹¤'
    },
    'smoking_experience': {
        # [ìˆ˜ì •] ë°ì´í„°ê°€ "ì¼ë°˜ ë‹´ë°°", "ì „ìžë‹´ë°°" í˜•íƒœì´ë¯€ë¡œ í•´ë‹¹ ë‹¨ì–´ í¬í•¨
        'ìžˆìŒ': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'], 
        'í¡ì—°': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'],
        # ë¶€ì • í‚¤ì›Œë“œëŠ” í™•ì‹¤í•œ ê²ƒë§Œ
        'ì—†ìŒ': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
        'ë¹„í¡ì—°': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
    },
    'drinking_experience': {
        # [ìˆ˜ì •] ìŒì£¼ ë°ì´í„°ë„ 'ì†Œì£¼', 'ë§¥ì£¼' í˜•íƒœì´ë¯€ë¡œ ì¢…ë¥˜ í¬í•¨
        'ìžˆìŒ': ['ì†Œì£¼', 'ë§¥ì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬', 'ìœ„ìŠ¤í‚¤', 'ì–‘ì£¼', 'ë§ˆì‹ ë‹¤', 'ìŒì£¼', 'ìˆ '],
        'ì—†ìŒ': ['ë§ˆì‹œì§€', 'ë¹„ìŒì£¼', 'ê¸ˆì£¼'],
    }
}

ARRAY_FIELDS = [
    "drinking_experience",
    "owned_electronics",
    "smoking_experience",
    "smoking_brand",
    "e_cigarette_experience"
]

def build_sql_from_structured_filters(filters: List[Dict]) -> Tuple[str, List]:
    """
    JSONB ë°ì´í„° íƒ€ìž…ì— ë§žì¶° ì •í™•í•œ SQL WHERE ì ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ TRIM ì œê±° ë° í•„ë“œ ë§¤í•‘ ì ìš©)
    """
    if not filters:
        return "", []

    conditions = []
    params = []
    CURRENT_YEAR = datetime.now().year

    for f in filters:
        raw_field = f.get("field")
        operator = f.get("operator")
        value = f.get("value")

        if not raw_field or not operator:
            continue

        field = FIELD_ALIAS_MAP.get(raw_field, raw_field)

        # --- [íŠ¹ìˆ˜ ì²˜ë¦¬] ë‚˜ì´ ê³„ì‚° ---
        if field == "birth_year" or raw_field == "age":
            if operator == "between" and isinstance(value, list) and len(value) == 2:
                age_start, age_end = value
                birth_year_end = CURRENT_YEAR - age_start
                birth_year_start = CURRENT_YEAR - age_end
                conditions.append(f"(structured_data->>'birth_year')::int BETWEEN %s AND %s")
                params.extend([birth_year_start, birth_year_end])
            continue
        
        # --- [ê°’ ë³€í™˜] ë§¤í•‘ëœ ê°’ìœ¼ë¡œ ë³€í™˜ ---
        final_value = value
        if field in VALUE_TRANSLATION_MAP:
            mapping = VALUE_TRANSLATION_MAP[field]
            if isinstance(value, list):
                converted_list = []
                for v in value:
                    mapped_v = mapping.get(v, v)
                    if isinstance(mapped_v, list):
                        converted_list.extend(mapped_v)
                    else:
                        converted_list.append(mapped_v)
                final_value = converted_list
            else:
                mapped_v = mapping.get(value, value)
                final_value = mapped_v

        # --- [ë¶„ê¸° 1] JSON ë°°ì—´(List) í•„ë“œ ì²˜ë¦¬ ---
        # ILIKEë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ë¶„ ë¬¸ìžì—´ ê²€ìƒ‰ (ë°°ì—´ -> ë¬¸ìžì—´ ë³€í™˜ í›„ ê²€ìƒ‰)
        if field in ARRAY_FIELDS:
            if not isinstance(final_value, list):
                final_value = [final_value]
            
            # "ì¼ë°˜ ë‹´ë°°" ë°ì´í„°ì—ì„œ "ì¼ë°˜"ë§Œ ìžˆì–´ë„ ì°¾ì„ ìˆ˜ ìžˆê²Œ ILIKE ì‚¬ìš©
            or_conditions = []
            for v in final_value:
                or_conditions.append(f"structured_data->>'{field}' ILIKE %s")
                params.append(f"%{v}%")
            
            if or_conditions:
                conditions.append(f"({' OR '.join(or_conditions)})")

        # --- [ë¶„ê¸° 2] ìˆ«ìží˜• í•„ë“œ ì²˜ë¦¬ ---
        elif field in ["children_count"]:
            field_sql = f"(structured_data->>'{field}')::numeric"
            if operator == "between" and isinstance(final_value, list) and len(final_value) == 2:
                conditions.append(f"{field_sql} BETWEEN %s AND %s")
                params.extend(final_value)
            elif operator == "gte":
                conditions.append(f"{field_sql} >= %s")
                params.append(final_value)
            elif operator == "lte":
                conditions.append(f"{field_sql} <= %s")
                params.append(final_value)
            elif operator == "eq":
                conditions.append(f"{field_sql} = %s")
                params.append(final_value)

        # --- [ë¶„ê¸° 3] ì¼ë°˜ ë¬¸ìžì—´ í•„ë“œ ì²˜ë¦¬ ---
        else:
            field_sql = f"structured_data->>'{field}'"

            if field == "family_size":
                if isinstance(final_value, list):
                    or_conditions = []
                    for v in final_value:
                        or_conditions.append(f"{field_sql} ILIKE %s")
                        params.append(f"%{v}%")
                    conditions.append(f"({' OR '.join(or_conditions)})")
                else:
                    conditions.append(f"{field_sql} ILIKE %s")
                    params.append(f"%{final_value}%")

            elif operator == "eq":
                if field in ["job_title_raw", "job_duty_raw"]:
                     conditions.append(f"{field_sql} ILIKE %s")
                     params.append(f"%{final_value}%")
                else:
                     conditions.append(f"{field_sql} = %s")
                     params.append(str(final_value))

            elif operator == "in" and isinstance(final_value, list) and final_value:
                str_values = [str(v) for v in final_value]
                placeholders = ','.join(['%s'] * len(str_values))
                conditions.append(f"{field_sql} IN ({placeholders})")
                params.extend(str_values)
                
            elif operator == "like":
                conditions.append(f"{field_sql} ILIKE %s")
                params.append(f"%{final_value}%")

    if not conditions:
        return "", []

    where_clause = " WHERE " + " AND ".join(conditions)
    return where_clause, params


def search_welcome_objective(
    filters: List[Dict],
    attempt_name: str = "êµ¬ì¡°í™”"
) -> Tuple[Set[str], Set[str]]:
    if not filters:
        logging.info(f"   Welcome {attempt_name}: í•„í„° ì—†ìŒ")
        return set(), set()

    try:
        with get_db_connection_context() as conn:
            if not conn:
                return set(), set()

            cur = conn.cursor()
            where_clause, params = build_sql_from_structured_filters(filters)

            if not where_clause:
                return set(), set()

            query = f"SELECT panel_id FROM welcome_meta2 {where_clause}"
            
            logging.info(f"  (SQL) ì‹¤í–‰: {query}")
            logging.info(f"  (SQL) íŒŒë¼ë¯¸í„°: {params}")

            cur.execute(query, tuple(params))
            
            results = {str(row[0]) for row in cur.fetchall()}
            cur.close()

            logging.info(f"  (SQL) ðŸ“ˆ 1ë‹¨ê³„ í•„í„°ë§ ê²°ê³¼: {len(results)}ëª…")

        return results, set()

    except Exception as e:
        logging.error(f"   Welcome {attempt_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
        return set(), set()

# ... (ì´í•˜ search_preference_conditions, filter_negative_conditions, initialize_embeddings ë“± ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ...
def search_preference_conditions(
    preference_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    candidate_panel_ids: Set[str],
    threshold: float = 0.45,
    top_k_per_keyword: int = 500
) -> Tuple[List[tuple], List[str]]:
    if not preference_keywords or not query_vectors or not candidate_panel_ids:
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])
    try:
        candidate_list = list(candidate_panel_ids)
        qdrant_filter = Filter(must=[FieldCondition(key="panel_id", match=MatchAny(any=candidate_list))])
        panel_scores: Dict[str, float] = {pid: 0.0 for pid in candidate_panel_ids}
        found_categories: List[str] = []
        for i, (keyword, vector) in enumerate(zip(preference_keywords, query_vectors)):
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, query_filter=qdrant_filter,
                limit=top_k_per_keyword, with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                category = result.payload.get('category', None)
                if not pid and 'metadata' in result.payload:
                    pid = result.payload['metadata'].get('panel_id')
                    if not category: category = result.payload['metadata'].get('category', None)
                if pid and str(pid) in panel_scores:
                    panel_scores[str(pid)] = max(panel_scores[str(pid)], result.score)
                    if category: found_categories.append(category)
        sorted_results = sorted(panel_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results, list(set(found_categories))
    except Exception as e:
        logging.error(f"Preference ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])

def filter_negative_conditions(
    panel_ids: Set[str],
    negative_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    threshold: float = 0.50
) -> Set[str]:
    if not negative_keywords or not query_vectors or not panel_ids: return panel_ids
    try:
        panel_ids_to_exclude = set()
        for vector in query_vectors:
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, limit=5000,
                with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                if not pid and 'metadata' in result.payload: pid = result.payload['metadata'].get('panel_id')
                if pid: panel_ids_to_exclude.add(str(pid))
        return panel_ids - panel_ids_to_exclude
    except Exception as e:
        logging.error(f"Negative í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return panel_ids

def find_negative_answer_ids(
    candidate_ids: Set[str],
    target_field: str,
    collection_name: str,
    is_welcome_collection: bool = False,
    threshold: float = 0.82 
) -> Set[str]:
    from mapping_rules import NEGATIVE_ANSWER_KEYWORDS
    
    negative_keywords = NEGATIVE_ANSWER_KEYWORDS.get(target_field)
    if not negative_keywords or not candidate_ids:
        return set()

    try:
        client = QdrantClient(url=os.getenv("QDRANT_HOST"))
        embeddings = initialize_embeddings()
        
        negative_vectors = embeddings.embed_documents(negative_keywords)
        
        ids_to_exclude = set()
        id_key_path = "metadata.panel_id" if is_welcome_collection else "panel_id"
        
        search_filter = Filter(
            must=[
                FieldCondition(key=id_key_path, match=MatchAny(any=list(candidate_ids)))
            ]
        )
        
        for neg_vec in negative_vectors:
            hits = client.search(
                collection_name=collection_name,
                query_vector=neg_vec,
                query_filter=search_filter,
                limit=len(candidate_ids), 
                score_threshold=threshold, 
                with_payload=[id_key_path]
            )
            
            for hit in hits:
                if is_welcome_collection:
                    pid = hit.payload.get('metadata', {}).get('panel_id')
                else:
                    pid = hit.payload.get('panel_id')
                
                if pid:
                    ids_to_exclude.add(pid)
        
        if ids_to_exclude:
            logging.info(f"   ðŸš« ë¶€ì • ë‹µë³€ í•„í„°ë§: {len(ids_to_exclude)}ëª… ì œì™¸ë¨ (í‚¤ì›Œë“œ: {negative_keywords})")
            
        return ids_to_exclude

    except Exception as e:
        logging.error(f"ë¶€ì • í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return set()

@lru_cache(maxsize=None)
def initialize_embeddings():
    try:
        return HuggingFaceEmbeddings(model_name="nlpai-lab/KURE-v1", model_kwargs={'device': 'cpu'})
    except Exception as e:
        logging.error(f"ìž„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def embed_keywords(keywords: List[str]) -> List[List[float]]:
    if not keywords: return []
    try:
        return initialize_embeddings().embed_documents(keywords)
    except Exception as e:
        logging.error(f"ìž„ë² ë”© ì‹¤íŒ¨: {e}")
        return []
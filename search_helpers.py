import os
import re
import logging
import threading
from typing import List, Set, Optional, Dict, Tuple
from datetime import datetime
from collections import defaultdict
from functools import lru_cache
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchAny, SearchParams
from langchain_huggingface import HuggingFaceEmbeddings

from db import get_db_connection_context
from mapping_rules import CATEGORY_MAPPING, get_field_mapping

# LLM í•„ë“œëª… -> ì‹¤ì œ DB í•„ë“œëª… ë§¤í•‘
FIELD_ALIAS_MAP = {
    "household_size": "family_size",  
    "age": "birth_year",              
    "job": "job_title_raw",           
    "region": "region_major"          
}

# ì‹¤ì œ ë°ì´í„°(ëª…ì‚¬í˜•)ë¥¼ í¬í•¨í•˜ë„ë¡ ë§¤í•‘ í‚¤ì›Œë“œ í™•ìž¥
VALUE_TRANSLATION_MAP = {
    'gender': {
        'ë‚¨ì„±': 'M', 'ì—¬ì„±': 'F', 'ë‚¨ìž': 'M', 'ì—¬ìž': 'F', 
        'male': 'M', 'female': 'F' # ì˜ì–´ ì¶”ê°€
    },
    'marital_status': {
        'ë¯¸í˜¼': 'ë¯¸í˜¼', 'ì‹±ê¸€': 'ë¯¸í˜¼', 'ê¸°í˜¼': 'ê¸°í˜¼', 'ê²°í˜¼': 'ê¸°í˜¼', 'ì´í˜¼': 'ì´í˜¼', 'ëŒì‹±': 'ì´í˜¼',
        'single': 'ë¯¸í˜¼', 'married': 'ê¸°í˜¼' # ì˜ì–´ ì¶”ê°€
    },
    'education_level': {
        'ê³ ì¡¸': ['ê³ ë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ê³ ë“±í•™êµ ì¡¸ì—…', 'ê³ ì¡¸'],
        'ê³ ë“±í•™êµ ì¡¸ì—…': ['ê³ ë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ê³ ë“±í•™êµ ì¡¸ì—…'],

        'ì¤‘ì¡¸ ì´í•˜': ['ê³ ë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ì¤‘í•™êµ ì¡¸ì—… ì´í•˜', 'ì¤‘í•™êµ ì¡¸ì—…', 'ì´ˆë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ë¬´í•™'],
        'ì¤‘í•™êµ ì¡¸ì—…': ['ê³ ë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ì¤‘í•™êµ ì¡¸ì—… ì´í•˜', 'ì¤‘í•™êµ ì¡¸ì—…'],

        'ëŒ€ì¡¸': ['ëŒ€í•™êµ ì¡¸ì—…', 'ëŒ€í•™ì› ìž¬í•™ ì´ìƒ', 'í•™ì‚¬', 'ì„ì‚¬', 'ë°•ì‚¬'],
        'ëŒ€í•™êµ ì¡¸ì—…': ['ëŒ€í•™êµ ì¡¸ì—…', 'ëŒ€í•™ì› ìž¬í•™ ì´ìƒ'],
        'ëŒ€í•™ì›': ['ëŒ€í•™ì› ìž¬í•™ ì´ìƒ'],

        'ì €í•™ë ¥': ['ê³ ë“±í•™êµ ì¡¸ì—… ì´í•˜', 'ì¤‘í•™êµ ì¡¸ì—… ì´í•˜', 'ê³ ë“±í•™êµ ì¡¸ì—…', 'ì¤‘í•™êµ ì¡¸ì—…', 'ì´ˆë“±í•™êµ ì¡¸ì—… ì´í•˜'],
        'ê³ í•™ë ¥': ['ëŒ€í•™êµ ì¡¸ì—…', 'ëŒ€í•™ì› ìž¬í•™ ì´ìƒ']
    },
    'smoking_experience': {
        'have_smoked': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'],
        'smoker': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'],
        
        'ìžˆìŒ': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'], 
        'í¡ì—°': ['ì¼ë°˜', 'ì „ìž', 'ê¸°íƒ€', 'í”¼ìš°ê³ ', 'í”¼ì› ', 'í¡ì—°', 'ì—°ì´ˆ', 'ê¶ë ¨'],
        
        'no': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
        'none': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
        'non_smoker': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
        'ì—†ìŒ': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
        'ë¹„í¡ì—°': ['í”¼ì›Œë³¸ ì ì´', 'ë¹„í¡ì—°'],
    },
    'drinking_experience': {
        'have_drink': ['ì†Œì£¼', 'ë§¥ì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬', 'ìœ„ìŠ¤í‚¤', 'ì–‘ì£¼', 'ì‚¬ì¼€', 'ê³¼ì¼ì¹µí…Œì¼ì£¼', 'ì €ë„ì£¼', 'ì¦ê²¨', 'ë§ˆì‹ ë‹¤'], # ì˜ì–´ ì¶”ê°€
        'drinker': ['ì†Œì£¼', 'ë§¥ì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬', 'ìœ„ìŠ¤í‚¤', 'ì–‘ì£¼', 'ì‚¬ì¼€', 'ê³¼ì¼ì¹µí…Œì¼ì£¼', 'ì €ë„ì£¼', 'ì¦ê²¨', 'ë§ˆì‹ ë‹¤'],
        'ìžˆìŒ': ['ì†Œì£¼', 'ë§¥ì£¼', 'ì™€ì¸', 'ë§‰ê±¸ë¦¬', 'ìœ„ìŠ¤í‚¤', 'ì–‘ì£¼', 'ì‚¬ì¼€', 'ê³¼ì¼ì¹µí…Œì¼ì£¼', 'ì €ë„ì£¼', 'ì¦ê²¨', 'ë§ˆì‹ ë‹¤'],
        
        'no': ['ìµœê·¼ 1ë…„ ì´ë‚´ ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ', 'ë§ˆì‹œì§€', 'ë¹„ìŒì£¼', 'ê¸ˆì£¼', 'ì•ŠìŒ'], # ì˜ì–´ ì¶”ê°€
        'ì—†ìŒ': ['ìµœê·¼ 1ë…„ ì´ë‚´ ìˆ ì„ ë§ˆì‹œì§€ ì•ŠìŒ', 'ë§ˆì‹œì§€', 'ë¹„ìŒì£¼', 'ê¸ˆì£¼', 'ì•ŠìŒ'],
    }
}

ARRAY_FIELDS = [
    "drinking_experience",
    "owned_electronics",
    "smoking_experience",
    "smoking_brand",
    "e_cigarette_experience"
]

def build_sql_from_structured_filters(filters: List[Dict]) -> Tuple[str, List]:
    """
    JSONB ë°ì´í„° íƒ€ìž…ì— ë§žì¶° ì •í™•í•œ SQL WHERE ì ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ TRIM ì œê±° ë° í•„ë“œ ë§¤í•‘ ì ìš©)
    """
    if not filters:
        return "", []

    conditions = []
    params = []
    CURRENT_YEAR = datetime.now().year

    for f in filters:
        raw_field = f.get("field")
        operator = f.get("operator")
        value = f.get("value")

        if not raw_field or not operator:
            continue

        field = FIELD_ALIAS_MAP.get(raw_field, raw_field)

        if operator == "not_null":
            # JSONB í•„ë“œ ë‚´ì— í•´ë‹¹ í‚¤ê°€ ì¡´ìž¬í•˜ê³ , nullì´ ì•„ë‹Œì§€ í™•ì¸
            base_condition = f"(structured_data->>'{field}' IS NOT NULL AND structured_data->>'{field}' != 'NaN')"
            
            # 2. 'ì—†ìŒ', 'í•´ë‹¹ ì—†ìŒ', 'ë¹„í¡ì—°' ë“± ë¬´ì˜ë¯¸í•œ í…ìŠ¤íŠ¸ë„ ì œì™¸ (SQL ë ˆë²¨)
            # ì •ê·œì‹ìœ¼ë¡œ 'ì—†ìŒ', 'ë¹„í¡ì—°', 'í”¼ìš°ì§€ ì•ŠìŒ' ë“±ì´ í¬í•¨ëœ ê²½ìš° ì œì™¸
            exclude_pattern = "ì—†ìŒ|ë¹„í¡ì—°|í•´ë‹¹ì‚¬í•­|í”¼ìš°ì§€|ê¸ˆì—°"
            refined_condition = f"({base_condition} AND structured_data->>'{field}' !~ '{exclude_pattern}')"
            
            conditions.append(refined_condition)
            continue 

        # --- ë‚˜ì´ ê³„ì‚° ---
        if field == "birth_year" or raw_field == "age":
            if operator == "between" and isinstance(value, list) and len(value) == 2:
                age_start, age_end = value
                birth_year_end = CURRENT_YEAR - age_start
                birth_year_start = CURRENT_YEAR - age_end
                conditions.append(f"(structured_data->>'birth_year')::int BETWEEN %s AND %s")
                params.extend([birth_year_start, birth_year_end])
            continue
        
        # --- ë§¤í•‘ëœ ê°’ìœ¼ë¡œ ë³€í™˜ ---
        final_value = value
        if field in VALUE_TRANSLATION_MAP:
            mapping = VALUE_TRANSLATION_MAP[field]
            if isinstance(value, list):
                converted_list = []
                for v in value:
                    mapped_v = mapping.get(v, v)
                    if isinstance(mapped_v, list):
                        converted_list.extend(mapped_v)
                    else:
                        converted_list.append(mapped_v)
                final_value = converted_list
            else:
                mapped_v = mapping.get(value, value)
                final_value = mapped_v

        # --- JSON ë°°ì—´(List) í•„ë“œ ì²˜ë¦¬ ---
        # ILIKEë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ë¶„ ë¬¸ìžì—´ ê²€ìƒ‰ (ë°°ì—´ -> ë¬¸ìžì—´ ë³€í™˜ í›„ ê²€ìƒ‰)
        if field in ARRAY_FIELDS:
            if not isinstance(final_value, list):
                final_value = [final_value]
            
            or_conditions = []
            for v in final_value:
                or_conditions.append(f"structured_data->>'{field}' ILIKE %s")
                params.append(f"%{v}%")
            
            if or_conditions:
                # ê¸ì • í‚¤ì›Œë“œ('ìˆ ')ê°€ ìžˆë”ë¼ë„ ë¶€ì •ì–´('ì•ŠìŒ')ê°€ ìžˆìœ¼ë©´ ì œì™¸í•˜ëŠ” ë¡œì§
                exclude_sql = ""
                
                # 1. ìŒì£¼ ê²½í—˜ (drinking_experience)
                if field == "drinking_experience":
                    # 'ë§ˆì‹œì§€', 'ì•ŠìŒ', 'ì—†ìŒ', 'ë¹„ìŒì£¼', 'ê¸ˆì£¼', 'ì•ˆ ë§ˆì‹¬' ë“±ì´ í¬í•¨ë˜ë©´ ì œì™¸
                    exclude_patterns = "ë§ˆì‹œì§€|ì•ŠìŒ|ì—†ìŒ|ë¹„ìŒì£¼|ê¸ˆì£¼|ì•ˆ\\s*ë§ˆì‹¬|ì „í˜€"
                    exclude_sql = f" AND structured_data->>'{field}' !~ '{exclude_patterns}'"
                
                # 2. í¡ì—° ê²½í—˜ (smoking_experience) - í˜¹ì‹œ ëª¨ë¥¼ ë¹„í¡ì—°ìž ë°ì´í„° ë°©ì§€
                elif field == "smoking_experience":
                    exclude_patterns = "í”¼ìš°ì§€|ì•ŠìŒ|ì—†ìŒ|ë¹„í¡ì—°|ê¸ˆì—°|ì•ˆ\\s*í”¼ì›€"
                    exclude_sql = f" AND structured_data->>'{field}' !~ '{exclude_patterns}'"

                elif field == "ott_count":
                    # '0ê°œ', 'ì•ˆ í•¨', 'ì—†ìŒ', 'ì´ìš© ì•ˆ' ë“±ì´ í¬í•¨ë˜ë©´ ì œì™¸
                    exclude_pattern = "0ê°œ|ì•ˆ\\s*í•¨|ì—†ìŒ|ì´ìš©\\s*ì•ˆ|ë³´ì§€\\s*ì•ŠìŒ"
                    conditions.append(f"({base_condition} AND structured_data->>'{field}' !~ '{exclude_pattern}')")

                # ê¸°ì¡´ OR ì¡°ê±´ ë’¤ì— AND ì œì™¸ ì¡°ê±´ ë¶™ì´ê¸°
                conditions.append(f"({' OR '.join(or_conditions)}){exclude_sql}")

        # --- ìˆ«ìží˜• í•„ë“œ ì²˜ë¦¬ ---
        elif field in ["children_count"]:
            field_sql = f"(structured_data->>'{field}')::numeric"
            if operator == "between" and isinstance(final_value, list) and len(final_value) == 2:
                conditions.append(f"{field_sql} BETWEEN %s AND %s")
                params.extend(final_value)
            elif operator == "gte":
                conditions.append(f"{field_sql} >= %s")
                params.append(final_value)
            elif operator == "lte":
                conditions.append(f"{field_sql} <= %s")
                params.append(final_value)
            elif operator == "eq":
                conditions.append(f"{field_sql} = %s")
                params.append(final_value)

        # --- ì¼ë°˜ ë¬¸ìžì—´ í•„ë“œ ì²˜ë¦¬ ---
        else:
            field_sql = f"structured_data->>'{field}'"

            if field == "family_size":
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° (ì˜ˆ: 2ì¸ ê°€êµ¬ OR 3ì¸ ê°€êµ¬)
                if isinstance(final_value, list):
                    or_conditions = []
                    for v in final_value:
                        or_conditions.append(f"{field_sql} ~ %s")
                        params.append(f"^{v}([^0-9]|$)") 
                    conditions.append(f"({' OR '.join(or_conditions)})")
                # ë‹¨ì¼ ê°’ì¸ ê²½ìš°
                else:
                    conditions.append(f"{field_sql} ~ %s")
                    params.append(f"^{final_value}([^0-9]|$)")

            elif operator == "eq":
                if field in ["job_title_raw", "job_duty_raw"]:
                     conditions.append(f"{field_sql} ILIKE %s")
                     params.append(f"%{final_value}%")
                else:
                     conditions.append(f"{field_sql} = %s")
                     params.append(str(final_value))

            elif operator == "in" and isinstance(final_value, list) and final_value:
                str_values = [str(v) for v in final_value]
                placeholders = ','.join(['%s'] * len(str_values))
                conditions.append(f"{field_sql} IN ({placeholders})")
                params.extend(str_values)
                
            elif operator == "like":
                conditions.append(f"{field_sql} ILIKE %s")
                params.append(f"%{final_value}%")

    if not conditions:
        return "", []

    where_clause = " WHERE " + " AND ".join(conditions)
    return where_clause, params


def search_welcome_objective(
    filters: List[Dict],
    attempt_name: str = "êµ¬ì¡°í™”"
) -> Tuple[Set[str], Set[str]]:
    if not filters:
        logging.info(f"   Welcome {attempt_name}: í•„í„° ì—†ìŒ")
        return set(), set()

    try:
        with get_db_connection_context() as conn:
            if not conn:
                return set(), set()

            cur = conn.cursor()
            where_clause, params = build_sql_from_structured_filters(filters)

            if not where_clause:
                return set(), set()

            query = f"SELECT panel_id FROM welcome_meta2 {where_clause}"
            
            logging.info(f"  (SQL) ì‹¤í–‰: {query}")
            logging.info(f"  (SQL) íŒŒë¼ë¯¸í„°: {params}")

            cur.execute(query, tuple(params))
            
            results = {str(row[0]) for row in cur.fetchall()}
            cur.close()

            logging.info(f"  (SQL) ðŸ“ˆ 1ë‹¨ê³„ í•„í„°ë§ ê²°ê³¼: {len(results)}ëª…")

        return results, set()

    except Exception as e:
        logging.error(f"   Welcome {attempt_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
        return set(), set()

def search_preference_conditions(
    preference_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    candidate_panel_ids: Set[str],
    threshold: float = 0.45,
    top_k_per_keyword: int = 500
) -> Tuple[List[tuple], List[str]]:
    if not preference_keywords or not query_vectors or not candidate_panel_ids:
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])
    try:
        candidate_list = list(candidate_panel_ids)
        qdrant_filter = Filter(must=[FieldCondition(key="panel_id", match=MatchAny(any=candidate_list))])
        panel_scores: Dict[str, float] = {pid: 0.0 for pid in candidate_panel_ids}
        found_categories: List[str] = []
        for i, (keyword, vector) in enumerate(zip(preference_keywords, query_vectors)):
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, query_filter=qdrant_filter,
                limit=top_k_per_keyword, with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                category = result.payload.get('category', None)
                if not pid and 'metadata' in result.payload:
                    pid = result.payload['metadata'].get('panel_id')
                    if not category: category = result.payload['metadata'].get('category', None)
                if pid and str(pid) in panel_scores:
                    panel_scores[str(pid)] = max(panel_scores[str(pid)], result.score)
                    if category: found_categories.append(category)
        sorted_results = sorted(panel_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results, list(set(found_categories))
    except Exception as e:
        logging.error(f"Preference ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])

def filter_negative_conditions(
    panel_ids: Set[str],
    negative_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    threshold: float = 0.50
) -> Set[str]:
    if not negative_keywords or not query_vectors or not panel_ids: return panel_ids
    try:
        panel_ids_to_exclude = set()
        for vector in query_vectors:
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, limit=5000,
                with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                if not pid and 'metadata' in result.payload: pid = result.payload['metadata'].get('panel_id')
                if pid: panel_ids_to_exclude.add(str(pid))
        return panel_ids - panel_ids_to_exclude
    except Exception as e:
        logging.error(f"Negative í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return panel_ids

def find_negative_answer_ids(
    candidate_ids: Set[str],
    target_field: str,
    collection_name: str,
    is_welcome_collection: bool = False,
    threshold: float = 0.82 
) -> Set[str]:
    from mapping_rules import NEGATIVE_ANSWER_KEYWORDS
    
    negative_keywords = NEGATIVE_ANSWER_KEYWORDS.get(target_field)
    if not negative_keywords or not candidate_ids:
        return set()

    try:
        client = QdrantClient(url=os.getenv("QDRANT_HOST"))
        embeddings = initialize_embeddings()
        
        negative_vectors = embeddings.embed_documents(negative_keywords)
        
        ids_to_exclude = set()
        id_key_path = "metadata.panel_id" if is_welcome_collection else "panel_id"
        
        search_filter = Filter(
            must=[
                FieldCondition(key=id_key_path, match=MatchAny(any=list(candidate_ids)))
            ]
        )
        
        for neg_vec in negative_vectors:
            hits = client.search(
                collection_name=collection_name,
                query_vector=neg_vec,
                query_filter=search_filter,
                limit=len(candidate_ids), 
                score_threshold=threshold, 
                with_payload=[id_key_path]
            )
            
            for hit in hits:
                if is_welcome_collection:
                    pid = hit.payload.get('metadata', {}).get('panel_id')
                else:
                    pid = hit.payload.get('panel_id')
                
                if pid:
                    ids_to_exclude.add(pid)
        
        if ids_to_exclude:
            logging.info(f"   ðŸš« ë¶€ì • ë‹µë³€ í•„í„°ë§: {len(ids_to_exclude)}ëª… ì œì™¸ë¨ (í‚¤ì›Œë“œ: {negative_keywords})")
            
        return ids_to_exclude

    except Exception as e:
        logging.error(f"ë¶€ì • í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return set()

@lru_cache(maxsize=None)
def initialize_embeddings():
    try:
        return HuggingFaceEmbeddings(model_name="nlpai-lab/KURE-v1", model_kwargs={'device': 'cpu'})
    except Exception as e:
        logging.error(f"ìž„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def embed_keywords(keywords: List[str]) -> List[List[float]]:
    if not keywords: return []
    try:
        return initialize_embeddings().embed_documents(keywords)
    except Exception as e:
        logging.error(f"ìž„ë² ë”© ì‹¤íŒ¨: {e}")
        return []
import os
import re
import logging
import threading
from typing import List, Set, Optional, Dict, Tuple
from datetime import datetime
from collections import defaultdict
from functools import lru_cache
from dotenv import load_dotenv

from qdrant_client import QdrantClient
from qdrant_client.http.models import Filter, FieldCondition, MatchAny, SearchParams
from langchain_huggingface import HuggingFaceEmbeddings

from db import get_db_connection_context
from mapping_rules import CATEGORY_MAPPING, get_field_mapping

# [ìˆ˜ì •] LLM í•„ë“œëª… -> ì‹¤ì œ DB í•„ë“œëª… ë§¤í•‘ (ì—¬ê¸°ì— household_size ì¶”ê°€!)
FIELD_ALIAS_MAP = {
    "household_size": "family_size",  # 1ì¸ ê°€êµ¬ -> ê°€ì¡± ìˆ˜
    "age": "birth_year",              # ë‚˜ì´ -> ì¶œìƒì—°ë„
    "job": "job_title_raw",           # ì§ì—…
    "region": "region_major"          # ì§€ì—­
}

# ê°’ ë³€í™˜ì´ í•„ìš”í•œ í•„ë“œë¥¼ ìœ„í•œ ë§¤í•‘
VALUE_TRANSLATION_MAP = {
    'gender': {
        'ë‚¨ì„±': 'M', 'ì—¬ì„±': 'F', 'ë‚¨ì': 'M', 'ì—¬ì': 'F',
    },
    'marital_status': {
        'ë¯¸í˜¼': 'ë¯¸í˜¼', 'ì‹±ê¸€': 'ë¯¸í˜¼', 'ê¸°í˜¼': 'ê¸°í˜¼', 'ê²°í˜¼': 'ê¸°í˜¼', 'ì´í˜¼': 'ì´í˜¼', 'ëŒì‹±': 'ì´í˜¼'
    }
}

ARRAY_FIELDS = [
    "drinking_experience",
    "owned_electronics",
    "smoking_experience",
    "smoking_brand",
    "e_cigarette_experience"
]

def build_sql_from_structured_filters(filters: List[Dict]) -> Tuple[str, List]:
    """
    JSONB ë°ì´í„° íƒ€ì…ì— ë§ì¶° ì •í™•í•œ SQL WHERE ì ˆì„ ìƒì„±í•©ë‹ˆë‹¤.
    (ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ TRIM ì œê±° ë° í•„ë“œ ë§¤í•‘ ì ìš©)
    """
    if not filters:
        return "", []

    conditions = []
    params = []
    CURRENT_YEAR = datetime.now().year

    for f in filters:
        raw_field = f.get("field")
        operator = f.get("operator")
        value = f.get("value")

        if not raw_field or not operator:
            continue

        # [í•µì‹¬ 1] í•„ë“œëª… ë§¤í•‘ ì ìš© (household_size -> family_size)
        field = FIELD_ALIAS_MAP.get(raw_field, raw_field)

        # --- [íŠ¹ìˆ˜ ì²˜ë¦¬] ë‚˜ì´ ê³„ì‚° (Age -> birth_year) ---
        # raw_fieldê°€ 'age'ì´ê±°ë‚˜ ë§¤í•‘ëœ fieldê°€ 'birth_year'ì¸ ê²½ìš°
        if field == "birth_year" or raw_field == "age":
            if operator == "between" and isinstance(value, list) and len(value) == 2:
                age_start, age_end = value
                birth_year_end = CURRENT_YEAR - age_start
                birth_year_start = CURRENT_YEAR - age_end
                # ì¸ë±ìŠ¤ í™œìš©ì„ ìœ„í•´ í˜•ë³€í™˜
                conditions.append(f"(structured_data->>'birth_year')::int BETWEEN %s AND %s")
                params.extend([birth_year_start, birth_year_end])
            continue
        
        # --- [ê°’ ë³€í™˜] ë§¤í•‘ëœ ê°’ìœ¼ë¡œ ë³€í™˜ ---
        final_value = value
        if field in VALUE_TRANSLATION_MAP:
            if isinstance(value, list):
                final_value = [VALUE_TRANSLATION_MAP[field].get(v, v) for v in value]
            else:
                final_value = VALUE_TRANSLATION_MAP[field].get(value, value)

        # --- [ë¶„ê¸° 1] JSON ë°°ì—´(List) í•„ë“œ ì²˜ë¦¬ ---
        if field in ARRAY_FIELDS:
            if operator == "in" and isinstance(final_value, list):
                placeholders = ','.join(['%s'] * len(final_value))
                conditions.append(f"structured_data->'{field}' ?| array[{placeholders}]")
                params.extend(final_value)
            elif operator == "eq":
                conditions.append(f"structured_data->'{field}' ? %s")
                params.append(str(final_value))

        # --- [ë¶„ê¸° 2] ìˆ«ìí˜• í•„ë“œ ì²˜ë¦¬ ---
        elif field in ["children_count"]:
            field_sql = f"(structured_data->>'{field}')::numeric"
            if operator == "between" and isinstance(final_value, list) and len(final_value) == 2:
                conditions.append(f"{field_sql} BETWEEN %s AND %s")
                params.extend(final_value)
            elif operator == "gte":
                conditions.append(f"{field_sql} >= %s")
                params.append(final_value)
            elif operator == "lte":
                conditions.append(f"{field_sql} <= %s")
                params.append(final_value)
            elif operator == "eq":
                conditions.append(f"{field_sql} = %s")
                params.append(final_value)

        # --- [ë¶„ê¸° 3] ì¼ë°˜ ë¬¸ìì—´ í•„ë“œ ì²˜ë¦¬ ---
        else:
            # [í•µì‹¬ 2] ì†ë„ ìµœì í™”ë¥¼ ìœ„í•´ TRIM() ì œê±° -> structured_data->>'field'
            field_sql = f"structured_data->>'{field}'"

            # family_size(1ì¸ ê°€êµ¬ ë“±)ëŠ” ìˆ«ì '1' ê²€ìƒ‰ ì‹œ '1ëª…', '1ì¸' ë“±ì„ ëª¨ë‘ ì°¾ì•„ì•¼ í•¨
            if field == "family_size":
                if isinstance(final_value, list):
                    or_conditions = []
                    for v in final_value:
                        or_conditions.append(f"{field_sql} ILIKE %s")
                        params.append(f"%{v}%")
                    conditions.append(f"({' OR '.join(or_conditions)})")
                else:
                    conditions.append(f"{field_sql} ILIKE %s")
                    params.append(f"%{final_value}%")

            elif operator == "eq":
                # job_title_raw ë“±ì€ ë¶€ë¶„ ì¼ì¹˜(ILIKE)ê°€ ì•ˆì „
                if field in ["job_title_raw", "job_duty_raw"]:
                     conditions.append(f"{field_sql} ILIKE %s")
                     params.append(f"%{final_value}%")
                else:
                     # ê·¸ ì™¸(ì§€ì—­, ì„±ë³„ ë“±)ëŠ” ì •í™• ì¼ì¹˜ (ì¸ë±ìŠ¤ í™œìš© ìµœì í™”)
                     conditions.append(f"{field_sql} = %s")
                     params.append(str(final_value))

            elif operator == "in" and isinstance(final_value, list) and final_value:
                str_values = [str(v) for v in final_value]
                placeholders = ','.join(['%s'] * len(str_values))
                conditions.append(f"{field_sql} IN ({placeholders})")
                params.extend(str_values)
                
            elif operator == "like":
                conditions.append(f"{field_sql} ILIKE %s")
                params.append(f"%{final_value}%")

    if not conditions:
        return "", []

    where_clause = " WHERE " + " AND ".join(conditions)
    return where_clause, params


def search_welcome_objective(
    filters: List[Dict],
    attempt_name: str = "êµ¬ì¡°í™”"
) -> Tuple[Set[str], Set[str]]:
    """
    Stage 1: LLM í•„í„° -> PostgreSQL í•„í„°ë§
    """
    if not filters:
        logging.info(f"   Welcome {attempt_name}: í•„í„° ì—†ìŒ")
        return set(), set()

    try:
        with get_db_connection_context() as conn:
            if not conn:
                return set(), set()

            cur = conn.cursor()
            where_clause, params = build_sql_from_structured_filters(filters)

            if not where_clause:
                return set(), set()

            query = f"SELECT panel_id FROM welcome_meta2 {where_clause}"
            
            logging.info(f"  (SQL) ì‹¤í–‰: {query}")
            logging.info(f"  (SQL) íŒŒë¼ë¯¸í„°: {params}")

            cur.execute(query, tuple(params))
            
            results = {str(row[0]) for row in cur.fetchall()}
            cur.close()

            logging.info(f"  (SQL) ğŸ“ˆ 1ë‹¨ê³„ í•„í„°ë§ ê²°ê³¼: {len(results)}ëª…")

        return results, set()

    except Exception as e:
        logging.error(f"   Welcome {attempt_name} ê²€ìƒ‰ ì‹¤íŒ¨: {e}", exc_info=True)
        return set(), set()


def search_preference_conditions(
    preference_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    candidate_panel_ids: Set[str],
    threshold: float = 0.45,
    top_k_per_keyword: int = 500
) -> Tuple[List[tuple], List[str]]:
    if not preference_keywords or not query_vectors or not candidate_panel_ids:
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])
    try:
        candidate_list = list(candidate_panel_ids)
        qdrant_filter = Filter(must=[FieldCondition(key="panel_id", match=MatchAny(any=candidate_list))])
        panel_scores: Dict[str, float] = {pid: 0.0 for pid in candidate_panel_ids}
        found_categories: List[str] = []
        for i, (keyword, vector) in enumerate(zip(preference_keywords, query_vectors)):
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, query_filter=qdrant_filter,
                limit=top_k_per_keyword, with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                category = result.payload.get('category', None)
                if not pid and 'metadata' in result.payload:
                    pid = result.payload['metadata'].get('panel_id')
                    if not category: category = result.payload['metadata'].get('category', None)
                if pid and str(pid) in panel_scores:
                    panel_scores[str(pid)] = max(panel_scores[str(pid)], result.score)
                    if category: found_categories.append(category)
        sorted_results = sorted(panel_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_results, list(set(found_categories))
    except Exception as e:
        logging.error(f"Preference ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return ([(pid, 0.0) for pid in candidate_panel_ids], [])

def filter_negative_conditions(
    panel_ids: Set[str],
    negative_keywords: List[str],
    query_vectors: List[List[float]],
    qdrant_client: QdrantClient,
    collection_name: str,
    threshold: float = 0.50
) -> Set[str]:
    if not negative_keywords or not query_vectors or not panel_ids: return panel_ids
    try:
        panel_ids_to_exclude = set()
        for vector in query_vectors:
            search_results = qdrant_client.search(
                collection_name=collection_name, query_vector=vector, limit=5000,
                with_payload=True, score_threshold=threshold
            )
            for result in search_results:
                pid = result.payload.get('panel_id')
                if not pid and 'metadata' in result.payload: pid = result.payload['metadata'].get('panel_id')
                if pid: panel_ids_to_exclude.add(str(pid))
        return panel_ids - panel_ids_to_exclude
    except Exception as e:
        logging.error(f"Negative í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return panel_ids

@lru_cache(maxsize=None)
def initialize_embeddings():
    try:
        return HuggingFaceEmbeddings(model_name="nlpai-lab/KURE-v1", model_kwargs={'device': 'cpu'})
    except Exception as e:
        logging.error(f"ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise

def embed_keywords(keywords: List[str]) -> List[List[float]]:
    if not keywords: return []
    try:
        return initialize_embeddings().embed_documents(keywords)
    except Exception as e:
        logging.error(f"ì„ë² ë”© ì‹¤íŒ¨: {e}")
        return []

def find_negative_answer_ids(
    candidate_ids: Set[str],
    target_field: str,
    collection_name: str,
    is_welcome_collection: bool = False,
    threshold: float = 0.82 # ìœ ì‚¬ë„ 0.82 ì´ìƒì´ë©´ 'ë¶€ì • ë‹µë³€'ìœ¼ë¡œ ê°„ì£¼
) -> Set[str]:
    """
    í›„ë³´êµ° ì¤‘ì—ì„œ 'ì—†ë‹¤', 'ëª¨ë¥´ê² ë‹¤' ë“± ë¶€ì •ì ì¸ ë‹µë³€ì„ í•œ íŒ¨ë„ì˜ IDë¥¼ ì°¾ìŠµë‹ˆë‹¤.
    """
    from mapping_rules import NEGATIVE_ANSWER_KEYWORDS
    
    # 1. í•´ë‹¹ í•„ë“œì— ëŒ€í•œ ë¶€ì • í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
    negative_keywords = NEGATIVE_ANSWER_KEYWORDS.get(target_field)
    if not negative_keywords or not candidate_ids:
        return set()

    try:
        client = QdrantClient(url=os.getenv("QDRANT_HOST"))
        embeddings = initialize_embeddings()
        
        # 2. ë¶€ì • í‚¤ì›Œë“œ ë²¡í„°í™” (ì˜ˆ: "ìŠ¤íŠ¸ë ˆìŠ¤ ì—†ë‹¤"ì˜ ë²¡í„°)
        # ì—¬ëŸ¬ í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ ê±¸ë¦¬ë©´ ë˜ë¯€ë¡œ ê°ê° ê²€ìƒ‰
        negative_vectors = embeddings.embed_documents(negative_keywords)
        
        ids_to_exclude = set()
        id_key_path = "metadata.panel_id" if is_welcome_collection else "panel_id"
        
        # 3. í›„ë³´êµ°(candidate_ids) ë‚´ì—ì„œë§Œ ê²€ìƒ‰í•˜ë„ë¡ í•„í„° ì„¤ì •
        search_filter = Filter(
            must=[
                FieldCondition(key=id_key_path, match=MatchAny(any=list(candidate_ids)))
            ]
        )
        
        # 4. ê° ë¶€ì • í‚¤ì›Œë“œì— ëŒ€í•´ ê²€ìƒ‰ ì‹¤í–‰
        for neg_vec in negative_vectors:
            hits = client.search(
                collection_name=collection_name,
                query_vector=neg_vec,
                query_filter=search_filter,
                limit=len(candidate_ids), # í›„ë³´êµ° ì „ì²´ ê²€ì‚¬
                score_threshold=threshold, # [ì¤‘ìš”] ì´ ì ìˆ˜ë³´ë‹¤ ë†’ìœ¼ë©´ 'ë¶€ì • ë‹µë³€'ìœ¼ë¡œ ê°„ì£¼
                with_payload=[id_key_path]
            )
            
            for hit in hits:
                if is_welcome_collection:
                    pid = hit.payload.get('metadata', {}).get('panel_id')
                else:
                    pid = hit.payload.get('panel_id')
                
                if pid:
                    ids_to_exclude.add(pid)
        
        if ids_to_exclude:
            logging.info(f"   ğŸš« ë¶€ì • ë‹µë³€ í•„í„°ë§: {len(ids_to_exclude)}ëª… ì œì™¸ë¨ (í‚¤ì›Œë“œ: {negative_keywords})")
            
        return ids_to_exclude

    except Exception as e:
        logging.error(f"ë¶€ì • í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return set()
import logging
import pandas as pd
import numpy as np
import asyncio
from typing import List, Dict, Any, Tuple, Set
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import Counter, defaultdict

from app.repositories.panel_repo import PanelRepository
from app.repositories.qpoll_repo import QpollRepository
from app.services.llm_service import LLMService
from app.core.semantic_router import router  
from app.utils.common import (
    find_target_columns_dynamic, 
    get_field_mapping, 
    clean_label, 
    calculate_distribution, 
    get_age_group, 
    truncate_text,
    filter_merged_panels
)
from app.constants.mapping import (
    FIELD_NAME_MAP, 
    QPOLL_FIELD_TO_TEXT, 
    WELCOME_OBJECTIVE_FIELDS,
    VALUE_TRANSLATION_MAP,
    QPOLL_FIELDS,
)

class AnalysisService:
    def __init__(self):
        self.panel_repo = PanelRepository()
        self.qpoll_repo = QpollRepository()
        self.llm_service = LLMService()

    async def get_insight_summary(self, panel_ids: List[str], question: str) -> Dict[str, Any]:
        """[Lite ëª¨ë“œ] ìš”ì•½"""
        target_ids = panel_ids[:1000]
        panels_data = self.panel_repo.get_panels_by_ids(target_ids)
        
        if not panels_data:
            return {"summary": "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.", "used_fields": []}

        df = pd.DataFrame(panels_data)
        target_columns = find_target_columns_dynamic(question)
        
        if not target_columns:
            stats_context = self._calculate_column_stats(df, ['gender', 'birth_year', 'region_major'])
            target_columns = ['ê¸°ë³¸ ì¸êµ¬í†µê³„']
        else:
            stats_context = self._calculate_column_stats(df, target_columns)

        summary_text = await self.llm_service.generate_insight_summary(question, stats_context)

        return {
            "summary": summary_text,
            "used_fields": target_columns
        }

    async def analyze_search_results(self, query: str, classification: Dict, panel_ids: List[str]) -> Tuple[Dict, str]:
        """[Pro ëª¨ë“œ] ì‹¬ì¸µ ë¶„ì„ ë° ì°¨íŠ¸ ìƒì„±"""
        if not panel_ids:
            return {"main_summary": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "charts": []}, "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"

        panels_data = self.panel_repo.get_panels_data_from_db(panel_ids[:5000])

        if not panels_data:
            return {"main_summary": "ë°ì´í„° ì—†ìŒ", "charts": []}, "ë°ì´í„° ì—†ìŒ"

        # ë©”ëª¨ë¦¬ ìƒ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§ ì ìš©
        filters = classification.get('demographic_filters', {}).copy()
        
        if 'region_major' in filters:
            filters['region'] = filters.pop('region_major')

        panels_data = filter_merged_panels(panels_data, filters)

        if not panels_data:
             return {"main_summary": "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (í•„í„°ë§ ë¨).", "charts": []}, "ì¡°ê±´ ë¶ˆì¼ì¹˜"

        # ì°¨íŠ¸ ìƒì„± ë¡œì§ 
        charts, used_fields = await self._generate_charts_optimized(query, classification, panels_data)

        # ìš”ì•½ ìƒì„±
        summary_text = await self._generate_result_overview(query, panel_ids, classification, panels_data)

        return {
            "query": query,
            "total_count": len(panels_data),
            "charts": charts
        }, summary_text

    async def _generate_charts_optimized(self, query: str, classification: Dict, panels_data: List[Dict]) -> Tuple[List[Dict], List[str]]:
        """
        ì™„ì „ ë™ì  ì°¨íŠ¸ ìƒì„± (ë°ì´í„° ê¸°ë°˜ êµì°¨ ë¶„ì„)
        êµì°¨ ë¶„ì„ì„ ìœ„í•œ Q-Poll ë°ì´í„° ì‚¬ì „ ë¡œë“œ í¬í•¨
        """
        
        # âœ… Step 0: Q-Poll ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ (êµì°¨ ë¶„ì„ì— í•„ìš”)
        panel_ids = [p['panel_id'] for p in panels_data]
        all_qpoll_fields = [field for field, _ in QPOLL_FIELDS]
        
        logging.info(f"   ğŸ“¥ [Data Load] Q-Poll ë°ì´í„° ë¡œë“œ ì‹œì‘: {len(panel_ids)}ëª…, {len(all_qpoll_fields)}ê°œ í•„ë“œ")
        
        qpoll_data = await asyncio.to_thread(
            self.qpoll_repo.get_responses_for_table, 
            panel_ids, 
            all_qpoll_fields
        )
        
        # panels_dataì— Q-Poll ë°ì´í„° ë³‘í•©
        merged_count = 0
        for panel in panels_data:
            pid = panel['panel_id']
            if pid in qpoll_data:
                panel.update(qpoll_data[pid])
                merged_count += 1
        
        logging.info(f"   âœ… [Data Load] Q-Poll ë°ì´í„° ë³‘í•© ì™„ë£Œ: {merged_count}/{len(panels_data)}ëª…")
        
        # ê¸°ì¡´ ë¡œì§ ì‹œì‘
        charts = []
        used_fields = []
        chart_tasks = []
        search_used_fields = set()

        # [A] ê²€ìƒ‰ í•„í„° ì‹ë³„
        demographic_filters = classification.get('demographic_filters', {})
        if 'age_range' in demographic_filters: 
            search_used_fields.add('birth_year')
        for key in demographic_filters:
            if key != 'age_range': 
                search_used_fields.add(key)
        
        semantic_conditions = classification.get('semantic_conditions', [])

        # [Logic 1] ìë…€ í•„í„° ìˆìœ¼ë©´ ê²°í˜¼ ìƒíƒœ ì œì™¸
        if 'children_count' in demographic_filters or 'children_count' in search_used_fields:
            search_used_fields.add('marital_status')

        # [Logic 2] ì§€ì—­ ì„¸ë¶„í™” + region_major ì™„ì „ ì œì™¸
        if 'region_minor' not in used_fields:
            region_in_filters = 'region_major' in demographic_filters or 'region' in demographic_filters
            
            region_values = [p.get('region_major') for p in panels_data if p.get('region_major')]
            unique_regions = set(region_values) if region_values else set()
            has_region_diversity = len(unique_regions) >= 2
            
            if region_in_filters or has_region_diversity:
                reasons = []
                if region_in_filters: reasons.append("í•„í„°")
                if has_region_diversity: reasons.append(f"ë¶„í¬({len(unique_regions)}ê°œ)")
                
                logging.info(f"   ğŸ—ºï¸ [Logic 2] region_minor ì¶”ê°€: {', '.join(reasons)}")
                chart_tasks.append({"type": "filter", "field": "region_minor", "priority": 0})
                used_fields.append("region_minor")
                
                search_used_fields.add('region_major')
                logging.info("   ğŸ—ºï¸ [Logic 2] region_major ì œì™¸ ì²˜ë¦¬")

        # [B] Target Field
        target_field = classification.get('target_field')
        if target_field and target_field != 'unknown' and target_field not in used_fields:
            if target_field not in search_used_fields:
                priority = 0
                
                if target_field in QPOLL_FIELD_TO_TEXT:
                    chart_tasks.append({"type": "qpoll", "field": target_field, "priority": priority})
                    used_fields.append(target_field)
                    
                    # [Logic 3] ê¸°ë³¸ ì¸êµ¬í†µê³„ (Priority 1ë¡œ ì„¤ì • - íƒ€ê²Ÿ ë‹¤ìŒ)
                    basic_demos = ['gender', 'birth_year']
                    
                    if 'region_minor' not in used_fields and 'region_major' not in search_used_fields:
                        basic_demos.append('region_major')
                    
                    for field in basic_demos:
                        if field not in used_fields and field not in search_used_fields:
                            chart_tasks.append({"type": "filter", "field": field, "priority": 1})
                            used_fields.append(field)
                else:
                    chart_tasks.append({"type": "filter", "field": target_field, "priority": priority})
                    used_fields.append(target_field)

        # [C] Semantic Conditions
        for condition in semantic_conditions:
            original_keyword = condition.get('original_keyword')
            if not original_keyword: continue
            
            field_info = router.find_closest_field(original_keyword)
            if field_info:
                found_field = field_info['field']
                if found_field in used_fields: continue
                
                prio = 1 if found_field not in search_used_fields else 3
                
                if found_field in QPOLL_FIELD_TO_TEXT:
                    chart_tasks.append({"type": "qpoll", "field": found_field, "priority": prio})
                else:
                    if found_field not in search_used_fields:
                        chart_tasks.append({"type": "filter", "field": found_field, "priority": prio})
                used_fields.append(found_field)

        # [Logic 4] ì°¨ëŸ‰ ì†Œìœ  ë¹„ìœ¨ 70% ì´ìƒ ì‹œ ì°¨ì¢… ë¶„ì„
        car_ownership_values = [p.get('car_ownership') for p in panels_data if p.get('car_ownership')]
        if car_ownership_values:
            flat_values = []
            car_map = VALUE_TRANSLATION_MAP.get('car_ownership', {})
            for v in car_ownership_values:
                cleaned = clean_label(v if isinstance(v, str) else str(v))
                flat_values.append(car_map.get(cleaned, cleaned))
            
            car_dist = calculate_distribution(flat_values)
            if car_dist.get('ìˆìŒ', 0) >= 70.0 or car_dist.get('ìˆìŒ(ìê°€)', 0) >= 70.0:
                if 'car_model_raw' not in used_fields:
                    chart_tasks.append({"type": "filter", "field": "car_model_raw", "priority": 2})
                    used_fields.append("car_model_raw")

        # âœ… Step 1: ê¸°ë³¸ ì°¨íŠ¸ ìƒì„± (ìµœëŒ€ 3ê°œë§Œ)
        chart_tasks.sort(key=lambda x: x['priority'])
        
        logging.info(f"   ğŸ“Š [Chart Tasks] ì´ {len(chart_tasks)}ê°œ íƒœìŠ¤í¬ ìƒì„±")
        
        with ThreadPoolExecutor(max_workers=5) as executor:
            futures = []
            for task in chart_tasks[:4]:  # ê¸°ë³¸ ì°¨íŠ¸ëŠ” ìµœëŒ€ 4ê°œë§Œ
                if task['type'] == 'qpoll':
                    futures.append(executor.submit(self._create_qpoll_chart, task['field']))
                else:
                    korean_name = FIELD_NAME_MAP.get(task['field'], task['field'])
                    futures.append(executor.submit(self._create_basic_chart, task['field'], korean_name, panels_data))
            
            for future in as_completed(futures):
                try:
                    res = future.result()
                    if res and res.get('chart_data'):
                        vals = list(res['chart_data'][0]['values'].values())
                        if vals and vals[0] > 95.0 and res.get('field') in search_used_fields:
                            logging.info(f"   â­ï¸ [Chart Skip] '{res.get('topic')}' (95% ì´ìƒ ì ë¦¼)")
                            continue 
                        charts.append(res)
                        logging.info(f"   âœ… [Chart Added] '{res.get('topic')}' (í˜„ì¬: {len(charts)}ê°œ)")
                except Exception as e:
                    logging.error(f"   âŒ [Chart Error] {e}", exc_info=True)

        # âœ… Step 2: êµì°¨ ë¶„ì„ ë¬´ì¡°ê±´ ìƒì„± (ìµœì†Œ 1ê°œ, ìµœëŒ€ 2ê°œ)
        pivot_field = target_field if (target_field and target_field in used_fields) else (used_fields[0] if used_fields else None)
        
        if pivot_field and len(charts) < 5:
            pivot_name = QPOLL_FIELD_TO_TEXT.get(pivot_field, FIELD_NAME_MAP.get(pivot_field, pivot_field))
            
            logging.info(f"   ğŸ¯ [Crosstab] í”¼ë²— í•„ë“œ: {pivot_field} ({pivot_name})")
            
            # ì™„ì „ ë™ì  ì¶• ì„ íƒ
            recommended_axes = self._select_dynamic_crosstab_axes(
                pivot_field, panels_data, search_used_fields, used_fields
            )
            
            if recommended_axes:
                crosstab_added = 0
                max_crosstab = min(2, 5 - len(charts))
                
                logging.info(f"   ğŸ¯ [Crosstab] ìƒì„± ëª©í‘œ: ìµœëŒ€ {max_crosstab}ê°œ")
                
                for ax_field, ax_name in recommended_axes:
                    if crosstab_added >= max_crosstab: break
                    
                    logging.info(f"   ğŸ”„ [Crosstab] ì‹œë„: {ax_name} x {pivot_name}")
                    
                    crosstab = self._create_crosstab_chart(
                        panels_data, ax_field, pivot_field, ax_name, pivot_name
                    )
                    
                    if crosstab and crosstab.get('chart_data'):
                        crosstab_values = crosstab['chart_data'][0]['values']
                        
                        # ê²€ì¦: ìµœì†Œ 2ê°œ ê·¸ë£¹, ë¶„í¬ ì°¨ì´ ìˆì–´ì•¼ í•¨
                        if len(crosstab_values) >= 2:
                            # ëª¨ë“  ê·¸ë£¹ì˜ ìƒìœ„ ë‹µë³€ì´ ë™ì¼í•˜ì§€ ì•Šì€ì§€ ì²´í¬
                            top_answers = []
                            for group_dist in crosstab_values.values():
                                if isinstance(group_dist, dict) and group_dist:
                                    top_answer = max(group_dist.items(), key=lambda x: x[1])[0]
                                    top_answers.append(top_answer)
                            
                            # ê·¸ë£¹ë³„ ë‹µë³€ì´ ë‹¤ë¥´ë©´ ì˜ë¯¸ ìˆëŠ” êµì°¨ ë¶„ì„
                            if len(set(top_answers)) > 1 or len(top_answers) == 0:  # ë¹ˆ ê²½ìš°ë„ í—ˆìš©
                                charts.append(crosstab)
                                crosstab_added += 1
                                logging.info(f"   ğŸ“Š [Crosstab Added] {ax_name} x {pivot_name} âœ…")
                            else:
                                logging.info(f"   â­ï¸ [Crosstab Skip] {ax_name} x {pivot_name} (ê·¸ë£¹ë³„ ì°¨ì´ ì—†ìŒ)")
                        else:
                            logging.info(f"   â­ï¸ [Crosstab Skip] {ax_name} x {pivot_name} (ê·¸ë£¹ ë¶€ì¡±: {len(crosstab_values)}ê°œ)")
                    else:
                        logging.info(f"   â­ï¸ [Crosstab Skip] {ax_name} x {pivot_name} (ë°ì´í„° ì—†ìŒ)")
                
                if crosstab_added == 0:
                    logging.warning("   âš ï¸ [Crosstab] êµì°¨ ë¶„ì„ ìƒì„± ì‹¤íŒ¨ - ëª¨ë“  ì¶•ì—ì„œ ë°ì´í„° ì—†ìŒ")
            else:
                logging.info("   âš ï¸ [Crosstab] ì¶”ì²œ ì¶• ì—†ìŒ")

        # âœ… Step 3: ì°¨íŠ¸ ë¶€ì¡± ì‹œ íŠ¹ì´ì  ìë™ ë°œêµ´
        current_crosstab_count = sum(1 for c in charts if c.get('chart_type') == 'crosstab')
        
        if len(charts) < 5:
            exclude_for_high_ratio = list(set(used_fields) | search_used_fields)
            
            if 'region_minor' in used_fields:
                exclude_for_high_ratio.append('region_major')
            
            if 'children_count' in demographic_filters or 'children_count' in search_used_fields:
                exclude_for_high_ratio.append('marital_status')

            max_high_ratio = 1 if current_crosstab_count >= 2 else (5 - len(charts))
            
            logging.info(f"   ğŸ” [High Ratio] íŠ¹ì´ì  íƒìƒ‰ ì‹œì‘ (ìµœëŒ€ {max_high_ratio}ê°œ)")
            
            high_ratio_charts = self._find_high_ratio_fields(
                panels_data, 
                exclude_fields=exclude_for_high_ratio, 
                max_charts=max_high_ratio
            )
            charts.extend(high_ratio_charts)

        logging.info(f"   ğŸ‰ [Result] ìµœì¢… ì°¨íŠ¸ ê°œìˆ˜: {len(charts)}ê°œ (êµì°¨ë¶„ì„: {current_crosstab_count}ê°œ)")
        
        return charts[:5], used_fields

    def _calculate_axis_importance(self, panels_data: List[Dict], target_field: str, 
                                    candidate_field: str) -> float:
        """
        íƒ€ê²Ÿ í•„ë“œì™€ í›„ë³´ ì¶• í•„ë“œ ê°„ì˜ ìƒê´€ê´€ê³„/ì¤‘ìš”ë„ë¥¼ ê³„ì‚°
        
        ë°˜í™˜ê°’: 0.0 ~ 1.0 (ë†’ì„ìˆ˜ë¡ ì˜ë¯¸ ìˆëŠ” êµì°¨ ë¶„ì„)
        """
        
        # 1. ë°ì´í„° ìˆ˜ì§‘
        pairs = []
        for panel in panels_data:
            target_val = panel.get(target_field)
            axis_val = panel.get(candidate_field)
            
            if target_val and axis_val:
                # birth_yearëŠ” ì—°ë ¹ëŒ€ë¡œ ë³€í™˜
                if candidate_field == 'birth_year':
                    axis_val = get_age_group(axis_val)
                
                # ë¦¬ìŠ¤íŠ¸ ê°’ ì²˜ë¦¬
                if isinstance(target_val, list):
                    target_val = ', '.join(map(str, target_val))
                if isinstance(axis_val, list):
                    axis_val = ', '.join(map(str, axis_val))
                
                pairs.append((str(target_val), str(axis_val)))
        
        if len(pairs) < 10:
            return 0.0
        
        # 2. êµì°¨í‘œ ìƒì„±
        cross_table = defaultdict(lambda: defaultdict(int))
        
        for target_val, axis_val in pairs:
            cross_table[axis_val][target_val] += 1
        
        # 3. ë‹¤ì–‘ì„± ì ìˆ˜
        axis_diversity = len(cross_table)
        target_diversity = len(set(t for t, _ in pairs))
        
        if axis_diversity < 2 or axis_diversity > 20:
            return 0.0
        
        if target_diversity < 2:
            return 0.0
        
        # 4. ë¶„í¬ ê· í˜•ë„ (ì—”íŠ¸ë¡œí”¼)
        total_count = len(pairs)
        entropy_score = 0.0
        
        for axis_val, target_counts in cross_table.items():
            axis_ratio = sum(target_counts.values()) / total_count
            
            group_total = sum(target_counts.values())
            group_entropy = 0.0
            
            for target_val, count in target_counts.items():
                p = count / group_total
                if p > 0:
                    group_entropy -= p * np.log2(p)
            
            entropy_score += axis_ratio * group_entropy
        
        # 5. ì°¨ë³„ì„± ì ìˆ˜ (ê° ì¶• ê·¸ë£¹ì´ ì„œë¡œ ë‹¤ë¥¸ íƒ€ê²Ÿ ë¶„í¬ë¥¼ ê°€ì§€ëŠ”ê°€?)
        target_distributions = []
        for axis_val in cross_table.keys():
            target_counts = cross_table[axis_val]
            total = sum(target_counts.values())
            
            dist = {t: (c / total) for t, c in target_counts.items()}
            target_distributions.append(dist)
        
        if len(target_distributions) < 2:
            variance_score = 0.0
        else:
            all_targets = set()
            for dist in target_distributions:
                all_targets.update(dist.keys())
            
            variance_sum = 0.0
            for target in all_targets:
                values = [dist.get(target, 0.0) for dist in target_distributions]
                variance_sum += np.var(values)
            
            variance_score = variance_sum / len(all_targets)
        
        # 6. ìµœì¢… ì ìˆ˜ ê³„ì‚°
        diversity_penalty = 1.0
        if axis_diversity < 3:
            diversity_penalty = axis_diversity / 3.0
        elif axis_diversity > 10:
            diversity_penalty = 10.0 / axis_diversity
        
        max_entropy = np.log2(target_diversity)
        normalized_entropy = entropy_score / max_entropy if max_entropy > 0 else 0.0
        
        normalized_variance = min(variance_score * 10, 1.0)
        
        final_score = (
            0.3 * diversity_penalty +
            0.3 * normalized_entropy +
            0.4 * normalized_variance
        )
        
        return final_score

    def _select_dynamic_crosstab_axes(self, target_field: str, panels_data: List[Dict], 
                                       search_used_fields: Set[str], used_fields: List[str]) -> List[Tuple[str, str]]:
        """
        ì™„ì „ ë™ì ìœ¼ë¡œ êµì°¨ ë¶„ì„ ì¶•ì„ ì„ íƒ (í•˜ë“œì½”ë”© ì—†ìŒ)
        [ìˆ˜ì •] ê²€ìƒ‰ ì¡°ê±´ì— í¬í•¨ëœ í•„ë“œë¼ë„, ë‚´ë¶€ ë¶„í¬ê°€ ë‹¤ì–‘í•˜ë©´(2ê°œ ê·¸ë£¹ ì´ìƒ) ì¶•ìœ¼ë¡œ í—ˆìš©
        """
        
        logging.info(f"   ğŸ” [Dynamic Crosstab] íƒ€ê²Ÿ: {target_field}")
        
        # 1. ëª¨ë“  ê°€ëŠ¥í•œ ì¶• í›„ë³´ ìˆ˜ì§‘
        all_candidate_fields = []
        
        # Welcome ì •í˜• ë°ì´í„° í•„ë“œ
        for field, name in WELCOME_OBJECTIVE_FIELDS:
            # âœ… ìˆ˜ì •ë¨: ì´ë¯¸ 'ì°¨íŠ¸'ë¡œ ê·¸ë ¤ì§„(used_fields) ê²ƒë§Œ ì œì™¸í•˜ê³ , 
            # ê²€ìƒ‰ ì¡°ê±´(search_used_fields)ì— ìˆë”ë¼ë„ í›„ë³´êµ°ì— í¬í•¨ì‹œí‚´.
            # (ë‹¨ì¼ ê°’ì¸ì§€ ì—¬ë¶€ëŠ” ë’¤ì˜ _calculate_axis_importanceì—ì„œ axis_diversity < 2 ë¡œ ê±¸ëŸ¬ì§)
            if field in used_fields:
                continue
            
            if field == target_field:
                continue
            
            all_candidate_fields.append((field, name))
        
        # Q-Poll í•„ë“œë„ í›„ë³´ì— ì¶”ê°€ (íƒ€ê²Ÿì´ ì •í˜• ë°ì´í„°ì¸ ê²½ìš°ë§Œ)
        if target_field not in QPOLL_FIELD_TO_TEXT:
            for field, desc in QPOLL_FIELDS:
                # âœ… ìˆ˜ì •ë¨: ì—¬ê¸°ë„ ë™ì¼í•˜ê²Œ search_used_fields ì¡°ê±´ ì œê±°
                if field in used_fields: 
                    continue
                
                if field == target_field:
                    continue
                
                all_candidate_fields.append((field, desc))
        
        if not all_candidate_fields:
            logging.info("   âš ï¸ [Dynamic Crosstab] í›„ë³´ ì¶• ì—†ìŒ")
            return []
        
        # 2. ê° í›„ë³´ ì¶•ì˜ ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
        axis_scores = []
        
        for candidate_field, candidate_name in all_candidate_fields:
            try:
                # _calculate_axis_importance ë‚´ë¶€ì—ì„œ ë‹¤ì–‘ì„±(diversity)ì´ 2 ë¯¸ë§Œì´ë©´ 0ì  ì²˜ë¦¬í•˜ë¯€ë¡œ
                # "20ëŒ€"ë§Œ ê²€ìƒ‰í•´ì„œ ë°ì´í„°ê°€ 20ëŒ€ë¿ì´ë¼ë©´ ìë™ìœ¼ë¡œ íƒˆë½ë¨.
                # "20ëŒ€, 30ëŒ€"ë¥¼ ê²€ìƒ‰í–ˆë‹¤ë©´ ë‹¤ì–‘ì„±ì´ 2ê°€ ë˜ì–´ ì‚´ì•„ë‚¨ìŒ.
                score = self._calculate_axis_importance(panels_data, target_field, candidate_field)
                
                if score > 0.1:
                    axis_scores.append({
                        'field': candidate_field,
                        'name': candidate_name,
                        'score': score
                    })
                    logging.info(f"      ğŸ“Š {candidate_name}: {score:.3f}")
            except Exception as e:
                logging.warning(f"      âŒ {candidate_name}: ê³„ì‚° ì‹¤íŒ¨ ({e})")
                continue
        
        # 3. ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        axis_scores.sort(key=lambda x: x['score'], reverse=True)
        
        # 4. ìƒìœ„ 4ê°œ ì„ íƒ
        selected_axes = [(a['field'], a['name']) for a in axis_scores[:4]]
        
        if axis_scores:
            top_scores_str = ', '.join([f"{a['name']}({a['score']:.2f})" for a in axis_scores[:4]])
            logging.info(f"   âœ… [Dynamic Crosstab] ì„ íƒëœ ì¶•: [{top_scores_str}]")
        
        return selected_axes

    def _create_crosstab_chart(self, panels_data, field1, field2, name1, name2) -> Dict:
        """êµì°¨ ë¶„ì„ ì°¨íŠ¸ ìƒì„± (ë¦¬ìŠ¤íŠ¸ íƒ€ì… ì§€ì› ìˆ˜ì •)"""
        crosstab = {}
        
        # [ìˆ˜ì • 1] ì¶• ë°ì´í„° ìˆ˜ì§‘ (ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™” ë° ì •ì œ)
        vals1 = []
        for p in panels_data:
            val = p.get(field1)
            if not val: continue

            # ì—°ë ¹ëŒ€ ë³€í™˜
            if field1 == 'birth_year': 
                val = get_age_group(val)
            
            # ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬ (Flatten: ['A', 'B'] -> 'A', 'B'ë¡œ ë¶„ë¦¬)
            if isinstance(val, list):
                vals1.extend([clean_label(v) for v in val])
            else:
                vals1.append(clean_label(val))
        
        if not vals1: 
            return {}
        
        # ìƒìœ„ 5ê°œ ê·¸ë£¹ ì¶”ì¶œ
        top_groups = [k for k, v in Counter(vals1).most_common(5)]

        for group in top_groups:
            group_panels = []
            
            # [ìˆ˜ì • 2] ê·¸ë£¹ë³„ íŒ¨ë„ í•„í„°ë§ (ë¦¬ìŠ¤íŠ¸ í¬í•¨ ì—¬ë¶€ í™•ì¸)
            for p in panels_data:
                p_val1 = p.get(field1)
                
                # ë¹„êµ ê°’ ì „ì²˜ë¦¬
                if field1 == 'birth_year': 
                    p_val1 = get_age_group(p_val1)
                
                is_match = False
                if isinstance(p_val1, list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš°: í•´ë‹¹ ê·¸ë£¹ í‚¤ì›Œë“œê°€ ë¦¬ìŠ¤íŠ¸ ì•ˆì— ìˆëŠ”ì§€ í™•ì¸
                    # (ë°ì´í„° ì •ì œ í›„ ë¹„êµ)
                    cleaned_list = [clean_label(x) for x in p_val1]
                    if str(group) in cleaned_list:
                        is_match = True
                else:
                    # ë‹¨ì¼ ê°’ì¸ ê²½ìš°: ë¬¸ìì—´ ì¼ì¹˜ í™•ì¸
                    if p_val1 and str(clean_label(p_val1)) == str(group):
                        is_match = True
            
                if is_match:
                    group_panels.append(p)
            
            # í”¼ë²— ë°ì´í„° ìˆ˜ì§‘ (ê¸°ì¡´ ë¡œì§)
            vals2 = []
            for p in group_panels:
                v = p.get(field2)
                if v:
                    if isinstance(v, list): 
                        vals2.extend(v)
                    else: 
                        vals2.append(v)
            
            if vals2:
                dist = calculate_distribution([clean_label(v) for v in vals2])
                crosstab[str(group)] = dict(sorted(dist.items(), key=lambda x: x[1], reverse=True)[:5])

        if not crosstab: 
            return {}

        return {
            "topic": f"{name1}ë³„ {name2} ë¶„í¬",
            "description": f"'{name1}'ì— ë”°ë¥¸ '{name2}' ì‘ë‹µ ì°¨ì´ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.",
            "chart_type": "crosstab",
            "chart_data": [{"label": f"{name1}ë³„ {name2}", "values": crosstab}],
            "fields": [field1, field2]
        }

    def _find_high_ratio_fields(self, panels_data, exclude_fields, max_charts) -> List[Dict]:
        """íŠ¹ì´ì (High Ratio) í•„ë“œ ìë™ ë°œêµ´"""
        results = []
        candidates = [f for f in WELCOME_OBJECTIVE_FIELDS if f[0] not in exclude_fields]
        
        for field, kname in candidates:
            if len(results) >= max_charts: break
            
            vals = []
            for p in panels_data:
                v = p.get(field)
                if v:
                    if isinstance(v, list): vals.extend(v)
                    else: vals.append(v)
            
            if not vals: continue
            dist = calculate_distribution([clean_label(str(x)) for x in vals])
            
            if not dist: continue
            top_k, top_v = sorted(dist.items(), key=lambda x: x[1], reverse=True)[0]
            
            if 40.0 <= top_v < 95.0:
                results.append({
                    "topic": f"{kname} íŠ¹ì§•",
                    "description": f"ì „ì²´ì˜ {top_v}%ê°€ '{top_k}'ì…ë‹ˆë‹¤.",
                    "ratio": f"{top_v}%",
                    "chart_data": [{"label": kname, "values": dict(list(dist.items())[:10])}],
                    "field": field
                })
        
        return results

    def _create_basic_chart(self, field_name: str, korean_name: str, panels_data: List[Dict]) -> Dict:
        """ì¼ë°˜ í•„ë“œ(DB) ì°¨íŠ¸ ë°ì´í„° ìƒì„±"""
        if field_name in ["children_count", "birth_year"]:
            distribution = self.panel_repo.get_field_distribution(field_name)
        else:
            values = []
            for item in panels_data:
                val = item.get(field_name)
                if val:
                    if isinstance(val, list):
                        values.extend([clean_label(v) for v in val])
                    else:
                        values.append(clean_label(val))
            distribution = calculate_distribution(values)

        if not distribution: return {}
        sorted_items = sorted(distribution.items(), key=lambda x: x[1], reverse=True)[:10]
        final_dist = dict(sorted_items)
        top_k, top_v = list(final_dist.items())[0]
        
        return {
            "topic": f"{korean_name} ë¶„í¬",
            "description": f"ê°€ì¥ ë§ì€ ì‘ë‹µì€ '{top_k}'({top_v}%) ì…ë‹ˆë‹¤.",
            "ratio": f"{top_v}%",
            "chart_data": [{"label": korean_name, "values": final_dist}],
            "field": field_name
        }

    def _create_qpoll_chart(self, field_name: str) -> Dict:
        """Q-Poll ì°¨íŠ¸"""
        distribution = self.qpoll_repo.get_distribution(field_name)
        if not distribution: return {}
        question_text = QPOLL_FIELD_TO_TEXT.get(field_name, field_name)
        top_k, top_v = list(distribution.items())[0]
        return {
            "topic": question_text,
            "description": f"ê°€ì¥ ë§ì€ ì‘ë‹µì€ '{top_k}'({top_v}%) ì…ë‹ˆë‹¤.",
            "ratio": f"{top_v}%",
            "chart_data": [{"label": question_text, "values": distribution}],
            "field": field_name
        }

    def _calculate_column_stats(self, df: pd.DataFrame, columns: List[str]) -> str:
        report = []
        for col in columns:
            if col not in df.columns: continue
            try:
                valid = df[col].dropna()
                if valid.empty: continue
                if valid.apply(lambda x: isinstance(x, list)).any():
                    counts = valid.explode().value_counts().head(5)
                else:
                    counts = valid.value_counts().head(5)
                korean_name = FIELD_NAME_MAP.get(col, col)
                lines = [f"[{korean_name}]"]
                for val, count in counts.items():
                    pct = (count / len(df)) * 100
                    lines.append(f"- {val}: {pct:.1f}%")
                report.append("\n".join(lines))
            except: pass
        return "\n\n".join(report)

    async def _generate_result_overview(self, query: str, panel_ids: List[str], classification: Dict, panels_data: List[Dict]) -> str:
        """
        ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
        """
        if not panels_data:
            return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        df = pd.DataFrame(panels_data[:1000])
        
        filter_summary = []
        filters = classification.get('demographic_filters', {})
        for k, v in filters.items():
            filter_summary.append(f"- {FIELD_NAME_MAP.get(k, k)}: {v}")
        
        semantic = classification.get('semantic_conditions', [])
        for s in semantic:
            filter_summary.append(f"- ì˜ë„ ì¡°ê±´: {s.get('original_keyword')}")
            
        filter_text = "\n".join(filter_summary)

        stats_context = []
        
        target_field = classification.get('target_field')
        if target_field and target_field in df.columns:
            counts = df[target_field].value_counts(normalize=True).head(3)
            if not counts.empty:
                items = [f"{k}({v*100:.1f}%)" for k, v in counts.items()]
                kname = FIELD_NAME_MAP.get(target_field, target_field)
                stats_context.append(f"[{kname} ë¶„í¬]: {', '.join(items)}")

        if 'birth_year' in df.columns:
            df['age_group'] = df['birth_year'].apply(lambda x: get_age_group(x) if x else None)
            age_counts = df['age_group'].value_counts(normalize=True).head(3)
            if not age_counts.empty:
                age_desc = [f"{age}({ratio*100:.1f}%)" for age, ratio in age_counts.items()]
                stats_context.append(f"[ì—°ë ¹ëŒ€ ë¶„í¬]: {', '.join(age_desc)}")

        if 'income_personal_monthly' in df.columns:
            top_income = df['income_personal_monthly'].value_counts(normalize=True).head(2) 
            if not top_income.empty:
                income_strs = [f"{k}({v*100:.1f}%)" for k, v in top_income.items()]
                stats_context.append(f"[ê°œì¸ ì›”ì†Œë“ ë¶„í¬]: {', '.join(income_strs)}")
        
        interest_fields = ['gender', 'region_major', 'job_title_raw', 'income_household_monthly', 'marital_status']
        for field in interest_fields:
            if field in df.columns:
                top = df[field].value_counts(normalize=True).head(1)
                if not top.empty:
                    val, ratio = top.index[0], top.values[0]
                    desc = f"{val}({ratio*100:.1f}%)"
                    if ratio >= 0.5: desc += " - ê³¼ë°˜ìˆ˜ ì´ìƒ"
                    kname = FIELD_NAME_MAP.get(field, field)
                    stats_context.append(f"[{kname}]: {desc}")

        full_context = f"""
ê²€ìƒ‰ ì¡°ê±´:
{filter_text}

ë°œê²¬ëœ íŠ¹ì§•:
{chr(10).join(stats_context)}
        """
        
        return await self.llm_service.generate_analysis_summary(query, full_context, len(panel_ids))





















import os
import json
import time
import asyncio
loop = asyncio.get_running_loop()
import logging
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional, Tuple, Dict, List
from fastapi.middleware.cors import CORSMiddleware

from llm import classify_query_keywords
from search_helpers import initialize_embeddings
from search import hybrid_search_parallel as hybrid_search
from analysis import analyze_search_results_optimized as analyze_search_results
from db import (
    log_search_query,
    get_db_connection_context,
    init_db,
    cleanup_db,
    get_qdrant_client
)
from qdrant_client.http.models import Filter, FieldCondition, MatchValue

# ë£¨íŠ¸ ë¡œê±° ì„¤ì •
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    datefmt='%Y-%m-%d %H:%M:%S')

# Uvicorn, FastAPI ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê±° ë ˆë²¨ ì„¤ì • (í•„ìš”ì‹œ)
logging.getLogger("uvicorn").setLevel(logging.WARNING)
logging.getLogger("fastapi").setLevel(logging.WARNING)
# --- ë¡œê¹… ì„¤ì • ---

app = FastAPI(title="Multi-Table Hybrid Search API v3 (Refactored)")

origins = [
    "http://localhost:5173",
    "http://localhost:3000",
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

def preload_models():
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ëª¨ë“  AI ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤."""
    logging.info("="*70)
    logging.info("ğŸ”„ ëª¨ë“  AI ëª¨ë¸ì„ ë¯¸ë¦¬ ë¡œë“œí•©ë‹ˆë‹¤...")
    initialize_embeddings()
    classify_query_keywords("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
    try:
        classify_query_keywords("ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸")
        logging.info("âœ… Claude (LLM) ëª¨ë¸ ì—°ê²° í™•ì¸ ì™„ë£Œ.")
    except Exception as e:
        logging.warning(f"âš ï¸  Claude (LLM) ëª¨ë¸ ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        logging.warning("   LLM ê¸°ëŠ¥ì´ ì‘ë™í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì§€ë§Œ, ì„œë²„ëŠ” ê³„ì† ì‹œì‘í•©ë‹ˆë‹¤.")
    logging.info("âœ… ëª¨ë“  AI ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    logging.info("="*70)

@app.on_event("startup")
async def startup_event():
    logging.info("ğŸš€ FastAPI ì‹œì‘...")
    init_db()
    preload_models()

@app.on_event("shutdown")
async def shutdown_event():
    logging.info("ğŸ§¹ FastAPI ì¢…ë£Œ... Connection Pool ì •ë¦¬")
    cleanup_db()


class SearchQuery(BaseModel):
    query: str
    search_mode: str = "all"

class SearchResponse(BaseModel):
    query: str
    classification: dict
    results: dict
    final_panel_ids: list[str]
    summary: dict

class AnalysisRequest(BaseModel):
    query: str
    search_mode: str = "weighted"

class AnalysisResponse(BaseModel):
    query: str
    total_count: int
    main_summary: str
    charts: list[dict]

def _prepare_display_fields(classification: Dict) -> List[Dict]:
    """
    [ë¦¬íŒ©í† ë§] ranked_keywordsë¡œë¶€í„° display_fieldsë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    display_fields_raw = []
    for kw_info in classification.get('ranked_keywords', [])[:5]:
        field = kw_info.get('field', '')
        description = kw_info.get('description', '')
        priority = kw_info.get('priority', 999)
        
        fields = [f.strip() for f in field.split(',')]
        
        for f in fields:
            if f:
                display_fields_raw.append({
                    'field': f,
                    'label': description,
                    'priority': priority
                })

    unique_display_fields_map = {}
    for item in display_fields_raw:
        if item['field'] not in unique_display_fields_map:
            unique_display_fields_map[item['field']] = item
    
    return list(unique_display_fields_map.values())


def _build_pro_mode_response(
    query_text: str,
    classification: Dict,
    search_results: Dict,
    display_fields: List[Dict],
    effective_search_mode: str
) -> Tuple[Dict, List[str]]:
    """
    Pro ëª¨ë“œì˜ ë³µì¡í•œ ì‘ë‹µ ë³¸ë¬¸ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    source_counts = {
        "welcome_objective_count": len(search_results['panel_id1']),
        "welcome_subjective_count": len(search_results['panel_id2']),
        "qpoll_count": len(search_results['panel_id3'])
    }
    
    summary = {
        "search_mode": effective_search_mode,
        "search_strategy": {
            "welcome_objective": bool(classification.get('welcome_keywords', {}).get('objective')),
            "welcome_subjective": bool(classification.get('welcome_keywords', {}).get('subjective')),
            "qpoll": bool(classification.get('qpoll_keywords', {}).get('keywords'))
        },
        "ranked_keywords": classification.get('ranked_keywords', [])
    }

    response = {
        "query": query_text,
        "classification": classification,
        "display_fields": display_fields,
        "source_counts": source_counts,
        "summary": summary,
    }

    if effective_search_mode == "all":
        # 'all' ëª¨ë“œëŠ” ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨
        response["results"] = {}
        panel_id_list = []
        for mode_name, mode_results in search_results['results'].items():
            response["results"][mode_name] = {
                "count": mode_results['count'],
                "panel_ids": mode_results['panel_ids'][:100],
                "top_scores": {
                    str(pid): mode_results['scores'].get(pid, 0)
                    for pid in mode_results['panel_ids'][:10]
                }
            }
            if 'weights' in mode_results:
                response["results"][mode_name]['weights'] = mode_results['weights']
        
        panel_id_list = search_results['results']['weighted']['panel_ids']
        response["final_panel_ids"] = panel_id_list[:100]

    else: # 'quota', 'weighted', 'union', 'intersection'
        final_panel_ids = search_results['final_panel_ids']
        match_scores = search_results['match_scores']
        
        response["results"] = {
            effective_search_mode: {
                "count": len(final_panel_ids),
                "panel_ids": final_panel_ids[:100],
                "top_scores": {str(pid): match_scores.get(pid, 0) for pid in final_panel_ids[:10]}
            }
        }
        response["final_panel_ids"] = final_panel_ids[:100]
        panel_id_list = final_panel_ids

    return response, panel_id_list

async def _perform_common_search(query_text: str, search_mode: str, mode: str) -> Tuple[Dict, List[str], Dict]:
    """
    /searchì™€ /search-and-analyzeê°€ ê³µìœ í•˜ëŠ” í•µì‹¬ ë¡œì§
    (LLM ë¶„ë¥˜, ë³‘ë ¬ ê²€ìƒ‰, ë¡œê·¸ ê¸°ë¡, ê²°ê³¼ í¬ë§·íŒ…)
    """
    logging.info(f"ğŸ” ê³µí†µ ê²€ìƒ‰ ì‹œì‘: {query_text} (ëª¨ë“œ: {search_mode}, ì‹¤í–‰: {mode})")
    
    # 1. LLM í‚¤ì›Œë“œ ë¶„ë¥˜
    classification = classify_query_keywords(query_text)
    logging.info(f"ğŸ¤– LLM ë¶„ë¥˜ ê²°ê³¼: {classification}")
    user_limit = classification.get('limit')
    effective_search_mode = "quota" if user_limit and user_limit > 0 else search_mode
    logging.info(f"ğŸ’¡ API: ê°ì§€ëœ Limit ê°’: {user_limit}")

    search_results = hybrid_search(
        classification,
        search_mode,
        user_limit
    )
    
    # 3. ê²€ìƒ‰ ë¡œê·¸ ê¸°ë¡
    total_count = len(search_results['final_panel_ids'])
    log_search_query(query_text, total_count)
    
    # 4. ì‘ë‹µ êµ¬ì„±
    display_fields = _prepare_display_fields(classification)
    panel_ids_for_analysis = search_results['final_panel_ids']
    
    # Lite ëª¨ë“œ ì‘ë‹µ ê°„ì†Œí™”
    if mode == "lite":
        lite_response = {
            "query": query_text,
            "classification": {
                "ranked_keywords": classification.get('ranked_keywords', []),
            },
            "display_fields": display_fields,
            "total_count": total_count,
            "final_panel_ids": panel_ids_for_analysis[:500], # í…Œì´ë¸” ì¡°íšŒë¥¼ ìœ„í•´ ìµœëŒ€ 500ê°œ
            "effective_search_mode": effective_search_mode
        }
        logging.info("âœ… ê³µí†µ ê²€ìƒ‰ ì™„ë£Œ (Lite ëª¨ë“œ ê°„ì†Œí™”)")
        return lite_response, panel_ids_for_analysis, classification

    # Pro ëª¨ë“œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    response, panel_id_list = _build_pro_mode_response(
        query_text,
        classification,
        search_results,
        display_fields,
        effective_search_mode
    )
    
    # ë¶„ì„ì„ ìœ„í•´ ìµœëŒ€ 5000ê°œ ID ì „ë‹¬
    panel_ids_for_analysis = panel_id_list[:5000]
    
    logging.info("âœ… ê³µí†µ ê²€ìƒ‰ ì™„ë£Œ (Pro ëª¨ë“œ ì „ì²´ ë°ì´í„°)")
    return response, panel_ids_for_analysis, classification


async def _get_ordered_table_data(
    ids_to_fetch: List[str], 
    fields_to_fetch: List[str] = None
) -> List[dict]:
    """
    DBì—ì„œ íŒ¨ë„ ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ë˜, ì…ë ¥ëœ id ë¦¬ìŠ¤íŠ¸ ìˆœì„œë¥¼ ë³´ì¥í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    - fields_to_fetchê°€ Noneì´ë©´ structured_data ì „ì²´ë¥¼,
    - fields_to_fetchê°€ ë¦¬ìŠ¤íŠ¸ë©´ í•´ë‹¹ í•„ë“œë§Œ ì„ íƒì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    """
    if not ids_to_fetch:
        return []

    table_data = []
    try:
        with get_db_connection_context() as conn:
            if not conn:
                raise Exception("DB ì—°ê²° ì‹¤íŒ¨")
            
            cur = conn.cursor()
            
            # 1. SQL ì¿¼ë¦¬ ì¤€ë¹„ (í•„ë“œ ì„ íƒ ë¶€ë¶„ ë™ì  êµ¬ì„±)
            if fields_to_fetch:
                field_selects = ", ".join([
                    f"structured_data->>'{field}' as \"{field}\""
                    for field in fields_to_fetch
                ])
                sql_query = f"SELECT panel_id, {field_selects} FROM welcome_meta2 WHERE panel_id = ANY(%s::text[])"
            else:
                sql_query = "SELECT panel_id, structured_data FROM welcome_meta2 WHERE panel_id = ANY(%s::text[])"

            # 2. DBì—ì„œ ë°ì´í„° ì¡°íšŒ
            cur.execute(sql_query, (ids_to_fetch,))
            results = cur.fetchall()
            columns = [desc[0] for desc in cur.description]

            # 3. ìˆœì„œ ì¬ì •ë ¬ì„ ìœ„í•œ ë§µ ìƒì„±
            fetched_data_map = {row[0]: row for row in results}

            # 4. ì…ë ¥ëœ ID ìˆœì„œëŒ€ë¡œ ê²°ê³¼ ì¬êµ¬ì„±
            for pid in ids_to_fetch:
                if pid in fetched_data_map:
                    row_data = fetched_data_map[pid]
                    if fields_to_fetch:
                        # Lite ëª¨ë“œ: íŠ¹ì • í•„ë“œë§Œ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬ ìƒì„±
                        data = {columns[i]: row_data[i] for i in range(len(columns))}
                    else:
                        # Pro ëª¨ë“œ: structured_data ì „ì²´ë¥¼ í¬í•¨
                        data = row_data[1] or {} # structured_dataê°€ nullì¼ ê²½ìš° ë¹ˆ dict
                        data['panel_id'] = pid
                    table_data.append(data)
            
            cur.close()
            
    except Exception as db_e:
        logging.error(f"Table Data ì¡°íšŒ ì‹¤íŒ¨: {db_e}", exc_info=True)
    
    return table_data


@app.post("/api/search")
async def search_panels(search_query: SearchQuery):
    """
    ğŸš€ Lite ëª¨ë“œ: ë¹ ë¥¸ ê²€ìƒ‰ (ì°¨íŠ¸ ë¶„ì„ ì—†ì´ í…Œì´ë¸” ë°ì´í„°ë§Œ ë°˜í™˜)
    """
    logging.info(f"ğŸš€ [Lite ëª¨ë“œ] ë¹ ë¥¸ ê²€ìƒ‰ ì‹œì‘: {search_query.query}")
    
    valid_modes = ["all", "weighted", "union", "intersection"]
    if search_query.search_mode not in valid_modes:
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid search_mode. Must be one of: {valid_modes}"
        )
    
    try:
        start_time = time.time()
        
        # 1. ê³µí†µ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ (mode="lite")
        lite_response, _, _ = await _perform_common_search(
            search_query.query, 
            search_query.search_mode,
            mode="lite"
        )
        
        search_time = time.time() - start_time
        logging.info(f"â±ï¸  [Lite ëª¨ë“œ] ê²€ìƒ‰ ì™„ë£Œ: {search_time:.2f}ì´ˆ")
        
        # 2. í…Œì´ë¸” ë°ì´í„° ì¡°íšŒ (ë¦¬íŒ©í† ë§ëœ í•¨ìˆ˜ ì‚¬ìš©)
        ids_to_fetch = lite_response['final_panel_ids']
        fields_to_fetch = [item['field'] for item in lite_response.get('display_fields', [])]
        
        db_start = time.time()
        logging.info(f"ğŸ“Š [Lite ëª¨ë“œ] í…Œì´ë¸” ë°ì´í„° ì¡°íšŒ ì‹œì‘ (ìµœëŒ€ {len(ids_to_fetch)}ê°œ)")
        
        table_data = await _get_ordered_table_data(ids_to_fetch, fields_to_fetch)
        
        db_time = time.time() - db_start
        logging.info(f"âœ… [Lite ëª¨ë“œ] í…Œì´ë¸” ë°ì´í„° {len(table_data)}ê°œ ì¡°íšŒ ì™„ë£Œ: {db_time:.2f}ì´ˆ")
        
        # 3. Lite ëª¨ë“œ ìµœì¢… ì‘ë‹µ êµ¬ì„±
        lite_response['tableData'] = table_data
        lite_response['mode'] = "lite" 
        del lite_response['final_panel_ids'] # ID ëª©ë¡ì€ ì‘ë‹µì—ì„œ ì œê±°
        
        total_time = time.time() - start_time
        logging.info(f"âœ… [Lite ëª¨ë“œ] ì „ì²´ ì™„ë£Œ: {total_time:.2f}ì´ˆ - ì´ {lite_response['total_count']}ê°œ ê²°ê³¼ ì¤‘ {len(table_data)}ê°œ í…Œì´ë¸” ë°ì´í„° ë°˜í™˜")
        
        return lite_response
        
    except HTTPException as e:
        raise e
    except Exception as e:
        logging.error(f"[Lite ëª¨ë“œ] /api/search ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/api/search-and-analyze")
async def search_and_analyze(request: AnalysisRequest):
    """
    ğŸ“Š Pro ëª¨ë“œ: ê²€ìƒ‰ + ì¸ì‚¬ì´íŠ¸ ë¶„ì„ (ì°¨íŠ¸ + í…Œì´ë¸” ë°ì´í„° ë°˜í™˜)
    """
    logging.info(f"ğŸ“Š [Pro ëª¨ë“œ] ê²€ìƒ‰ + ë¶„ì„ ì‹œì‘: {request.query}")
    
    valid_modes = ["all", "weighted", "union", "intersection"]
    if request.search_mode not in valid_modes:
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid search_mode. Must be one of: {valid_modes}"
        )
    
    try:
        # 1. ê³µí†µ ê²€ìƒ‰ í•¨ìˆ˜ í˜¸ì¶œ (mode="pro")
        response, panel_id_list, classification = await _perform_common_search(
            request.query, 
            request.search_mode,
            mode="pro"
        )
        
        # 2. ì°¨íŠ¸ ë°ì´í„° ìƒì„±
        logging.info("ğŸ“Š [Pro ëª¨ë“œ] ì°¨íŠ¸ ë°ì´í„° ìƒì„± ì‹œì‘")
        analysis_result, status_code = analyze_search_results(
            request.query,
            classification,
            panel_id_list
        )
        
        if status_code == 200:
            response['charts'] = analysis_result.get('charts', [])
            response['analysis_summary'] = analysis_result.get('main_summary', '')
            logging.info(f"âœ… [Pro ëª¨ë“œ] ì°¨íŠ¸ {len(response['charts'])}ê°œ ìƒì„± ì™„ë£Œ")
        else:
            response['charts'] = []
            response['analysis_summary'] = 'ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨'
            logging.warning("[Pro ëª¨ë“œ] ì°¨íŠ¸ ìƒì„± ì‹¤íŒ¨")
        
        # 3. Table Data ìƒì„± (ë¦¬íŒ©í† ë§ëœ í•¨ìˆ˜ ì‚¬ìš©)
        logging.info(f"ğŸ“Š [Pro ëª¨ë“œ] Table Data ìƒì„± ì‹œì‘ (íŒ¨ë„ {len(panel_id_list)}ê°œ ëŒ€ìƒ)")
        ids_to_fetch = response['final_panel_ids'] # Pro ëª¨ë“œëŠ” 100ê°œë§Œ
        
        # Pro ëª¨ë“œëŠ” fields_to_fetch=Noneìœ¼ë¡œ ì „ë‹¬ (ì „ì²´ structured_data ì¡°íšŒ)
        table_data = await _get_ordered_table_data(ids_to_fetch, fields_to_fetch=None)
        
        # 4. ìµœì¢… ì‘ë‹µ êµ¬ì„±
        response['tableData'] = table_data
        response['mode'] = 'pro'
        
        logging.info(f"âœ… [Pro ëª¨ë“œ] ì°¨íŠ¸ {len(response['charts'])}ê°œ, í…Œì´ë¸” ë°ì´í„° {len(table_data)}ê°œ ìƒì„± ì™„ë£Œ")
        
        return response
        
    except HTTPException as e:
        raise e
    except Exception as e:
        logging.error(f"[Pro ëª¨ë“œ] /api/search-and-analyze ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/api/debug/classify")
async def debug_classify(search_query: SearchQuery):
    """
    ì§ˆì˜ë¥¼ í‚¤ì›Œë“œë¡œ ë¶„ë¥˜ë§Œ í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜ (ê²€ìƒ‰ì€ ìˆ˜í–‰í•˜ì§€ ì•ŠìŒ)
    """
    try:
        classification = classify_query_keywords(search_query.query)
        return {
            "query": search_query.query,
            "classification": classification
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¶„ë¥˜ ì‹¤íŒ¨: {str(e)}")


async def _get_welcome_data(panel_id: str) -> Dict:
    """[ë¦¬íŒ©í† ë§] PostgreSQLì—ì„œ Welcome ë°ì´í„°ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    loop = asyncio.get_running_loop()
    
    def db_call():
        with get_db_connection_context() as conn:
            if not conn:
                raise HTTPException(status_code=500, detail="DB ì—°ê²° ì‹¤íŒ¨")
            
            cur = conn.cursor()
            cur.execute(
                "SELECT panel_id, structured_data FROM welcome_meta2 WHERE panel_id = %s",
                (panel_id,)
            )
            result = cur.fetchone()
            cur.close()
            
            if not result:
                raise HTTPException(status_code=404, detail=f"panel_id {panel_id}ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            panel_id_value, structured_data = result
            panel_data = {"panel_id": panel_id_value}
            if isinstance(structured_data, dict):
                panel_data.update(structured_data)
            return panel_data

    return await loop.run_in_executor(None, db_call)


async def _get_qpoll_data(panel_id: str) -> Dict:
    """[ë¦¬íŒ©í† ë§] Qdrantì—ì„œ QPoll ë°ì´í„°ë¥¼ ë¹„ë™ê¸°ì ìœ¼ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤."""
    loop = asyncio.get_running_loop()
    qpoll_data = {"qpoll_ì‘ë‹µ_ê°œìˆ˜": 0}

    def qdrant_call():
        try:
            qdrant_client = get_qdrant_client()
            if not qdrant_client:
                logging.warning("âš ï¸  Qdrant í´ë¼ì´ì–¸íŠ¸ ì—†ìŒ")
                return qpoll_data

            logging.info(f"ğŸ” QPoll ë°ì´í„° ì¡°íšŒ ì‹œì‘ (panel_id: {panel_id})")
            qpoll_results, _ = qdrant_client.scroll(
                collection_name="qpoll_vectors_v2",
                scroll_filter=Filter(must=[FieldCondition(key="panel_id", match=MatchValue(value=panel_id))]),
                limit=100, with_payload=True, with_vectors=False
            )

            if qpoll_results:
                logging.info(f"âœ… QPoll ì‘ë‹µ {len(qpoll_results)}ê°œ ë°œê²¬")
                for idx, point in enumerate(qpoll_results, 1):
                    if point.payload:
                        qpoll_data[f"qpoll_{idx:03d}_ì§ˆë¬¸"] = point.payload.get("question", "")
                        qpoll_data[f"qpoll_{idx:03d}_ì‘ë‹µ"] = point.payload.get("sentence", "")
                qpoll_data["qpoll_ì‘ë‹µ_ê°œìˆ˜"] = len(qpoll_results)
            else:
                logging.warning("âš ï¸  QPoll ì‘ë‹µ ì—†ìŒ")
            
            return qpoll_data

        except Exception as qpoll_error:
            logging.error(f"âŒ QPoll ì¡°íšŒ ì‹¤íŒ¨ (panel_id: {panel_id}): {qpoll_error}", exc_info=True)
            qpoll_data["qpoll_ì¡°íšŒ_ì˜¤ë¥˜"] = str(qpoll_error)
            return qpoll_data

    return await loop.run_in_executor(None, qdrant_call)


@app.get("/api/panels/{panel_id}")
async def get_panel_details(panel_id: str):
    """
    [ê°œì„ ] íŠ¹ì • panel_idì˜ íŒ¨ë„ ìƒì„¸ ì •ë³´ë¥¼ ë³‘ë ¬ë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.
    - Welcome(PostgreSQL)ê³¼ QPoll(Qdrant) ë°ì´í„°ë¥¼ ë™ì‹œì— ì¡°íšŒí•˜ì—¬ ì„±ëŠ¥ ê°œì„ 
    """
    try:
        logging.info(f"âš¡ï¸ íŒ¨ë„ ìƒì„¸ ì •ë³´ ë³‘ë ¬ ì¡°íšŒ ì‹œì‘ (panel_id: {panel_id})")
        
        # Welcome ë°ì´í„°ì™€ QPoll ë°ì´í„°ë¥¼ ë™ì‹œì— ì¡°íšŒ
        results = await asyncio.gather(
            _get_welcome_data(panel_id),
            _get_qpoll_data(panel_id),
            return_exceptions=True  # í•œìª½ì—ì„œ ì—ëŸ¬ê°€ ë‚˜ë„ ë‹¤ë¥¸ ìª½ì€ ê³„ì† ì§„í–‰
        )

        # ê²°ê³¼ ì·¨í•©
        panel_data, qpoll_data = {}, {}
        for result in results:
            if isinstance(result, HTTPException):
                raise result # 404 Not Found ë“±ì€ ì¦‰ì‹œ ë°˜í™˜
            elif isinstance(result, Exception):
                raise HTTPException(status_code=500, detail=f"ì¡°íšŒ ì¤‘ ë‚´ë¶€ ì˜¤ë¥˜ ë°œìƒ: {result}")
            
            if "qpoll_ì‘ë‹µ_ê°œìˆ˜" in result:
                qpoll_data = result
            else:
                panel_data = result

        panel_data.update(qpoll_data)
        return panel_data
        
    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"íŒ¨ë„ ìƒì„¸ ì¡°íšŒ ì‹¤íŒ¨: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")


@app.get("/")
def read_root():
    return {
        "service": "Multi-Table Hybrid Search & Analysis API",
        "version": "3.0 (Refactored)",
        "status": "running",
        "optimizations_applied": [
            "DB Connection Pool (psycopg2-pool)",
            "Parallel Search (ThreadPoolExecutor)",
            "DB Aggregate Queries (analysis_logic)"
        ],
        "endpoints": {
            "search": "/api/search (Lite)",
            "search_and_analyze": "/api/search-and-analyze (Pro)",
            "classify": "/api/debug/classify",
            "panel_detail": "/api/panels/{panel_id}",
            "health": "/health"
        }
    }

@app.get("/health")
def health_check():
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
    try:
        with get_db_connection_context() as conn:
            db_status = "ok" if conn else "error"
        
        return {
            "status": "healthy",
            "database": db_status
        }
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e)
        }